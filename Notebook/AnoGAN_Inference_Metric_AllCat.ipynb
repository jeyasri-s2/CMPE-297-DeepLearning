{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "AnoGAN_Inference_Metric_AllCat.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plodha/CMPE-297-DeepLearning/blob/main/Notebook/AnoGAN_Inference_Metric_AllCat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCzmSViMljTt"
      },
      "source": [
        "# Mount Drive and Set Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GImoCE9Uk0CF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb7484e-39a9-4553-bf2d-59146c5008d7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNhf6fvzk3Lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dde7395-21ac-48ab-d909-3ef46246a97b"
      },
      "source": [
        "!pip install torch==1.7.0 torchvision==0.5.0 tqdm opencv-python Pillow==8.0.1 tensorboardX==1.4"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (1.7.0)\n",
            "Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: Pillow==8.0.1 in /usr/local/lib/python3.6/dist-packages (8.0.1)\n",
            "Requirement already satisfied: tensorboardX==1.4 in /usr/local/lib/python3.6/dist-packages (1.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.4) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9XQVZ8yk5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87019d6a-5b22-4384-ed89-3e4e0bbc44cf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Dec  5 20:59:46 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvWbMV8Lk7mB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5755ec57-3f5e-4753-d527-6883852668ec"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  images  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktBKO9rWkpqi"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "import random\n",
        "import pandas as pd\n",
        "#import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "#from dataloader.dataloader import load_data\n",
        "#from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "#from networks import Generator, Discriminator\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "ngpu = 1\n",
        "os.makedirs(\"images\", exist_ok=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32g4Nghbkpqj"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "class MURA_dataset(Dataset):\n",
        "    '''\n",
        "    Dataset class for MURA dataset\n",
        "    Args:\n",
        "        - df: Dataframe with the first columns contains the path to the images\n",
        "        - root_dir: string contains path of  root directory\n",
        "        - transforms: Pytorch transform operations\n",
        "    '''\n",
        "\n",
        "    def __init__(self, df, root_dir, transforms=None):\n",
        "        #print(\"I am calling Mura dataset\")\n",
        "        self.df = df\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.df.iloc[idx, 0])\n",
        "        #print('img_name ',img_name)\n",
        "        img = cv2.imread(img_name)\n",
        "        #print('img shape ',img.shape)\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        if 'negative' in img_name: label = 0\n",
        "        else: label = 1\n",
        "\n",
        "        return img, label"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hqNZtKDkpqj"
      },
      "source": [
        "def transform(rotation, hflip, resize, totensor, normalize, centercrop, to_pil, gray):\n",
        "    options = []\n",
        "    if to_pil:\n",
        "        options.append(torchvision.transforms.ToPILImage())\n",
        "    if gray:\n",
        "        options.append(torchvision.transforms.Grayscale())\n",
        "    if rotation:\n",
        "        options.append(torchvision.transforms.RandomRotation(20))\n",
        "    if hflip:\n",
        "        options.append(torchvision.transforms.RandomHorizontalFlip())\n",
        "    if centercrop:\n",
        "        options.append(torchvision.transforms.CenterCrop(256))\n",
        "    if resize:\n",
        "        options.append(torchvision.transforms.Resize((32,32)))\n",
        "    if totensor:\n",
        "        options.append(torchvision.transforms.ToTensor())\n",
        "    # if True:\n",
        "    #     options.append(transforms.Lambda(lambda x: (x - x.min())/(x.max()-x.min())))\n",
        "    if normalize:\n",
        "        options.append(torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
        "    transform = torchvision.transforms.Compose(options)\n",
        "    return transform"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar_SQ2hhkpqj"
      },
      "source": [
        "def customDf(path, studyClass=None, studyType=None):\n",
        "    '''\n",
        "    Function to get custom csv based on class of study and type of study\n",
        "    Args:\n",
        "        - path(string): path to original csv\n",
        "        - studyClass(list): class of study, list must contains one of the following:\n",
        "            \"XR_ELBOW\",\n",
        "            \"XR_FINGER\",\n",
        "            \"XR_FOREARM\",\n",
        "            \"XR_HAND\",\n",
        "            \"XR_HUMERUS\",\n",
        "            \"XR_SHOULDER\",\n",
        "            \"XR_WRIST\"\n",
        "            if None, take all\n",
        "        - studyResult(list): Result of study, list must contains one of the following:\n",
        "            \"positive\", \"negative\"\n",
        "            if None, take all\n",
        "    '''\n",
        "    df = pd.read_csv(path, header=None)\n",
        "\n",
        "    if studyClass:\n",
        "        cond = df[0].str.contains(studyClass)\n",
        "        df = df[cond]\n",
        "    if studyType:\n",
        "        cond = df[0].str.contains(studyType)\n",
        "        df = df[cond]\n",
        "    return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-A8jeDGkpqj"
      },
      "source": [
        "#import pandas as pd\n",
        "#import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyMShdYgkpqj"
      },
      "source": [
        "#valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MZ4oVUFkpqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31716082-6ff4-4ce8-df8c-f0cd268b71c7"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGrV7RkUkpql"
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdSxrnohkpql"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umqs0EXGkpql"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, dim, zdim, nc):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.dim = dim\n",
        "        preprocess = nn.Sequential(\n",
        "            nn.Linear(zdim, 4 * 4 * 4 * dim),\n",
        "            nn.BatchNorm1d(4 * 4 * 4 * dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        block1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(4 * dim, 2 * dim, 2, stride=2),\n",
        "            nn.BatchNorm2d(2 * dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        block2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2 * dim, dim, 2, stride=2),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        deconv_out = nn.ConvTranspose2d(dim, nc, 2, stride=2)\n",
        "\n",
        "        self.preprocess = preprocess\n",
        "        self.block1 = block1\n",
        "        self.block2 = block2\n",
        "        self.deconv_out = deconv_out\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.preprocess(input)\n",
        "        output = output.view(-1, 4 * self.dim, 4, 4)\n",
        "        output = self.block1(output)\n",
        "        output = self.block2(output)\n",
        "        output = self.deconv_out(output)\n",
        "        output = self.tanh(output)\n",
        "        return output.view(-1, self.nc, 32, 32)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAqyGQqQkpql"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qfMbK1skpql"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, dim, zdim, nc, out_feat=False):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.dim = dim\n",
        "        main = nn.Sequential(\n",
        "            nn.Conv2d(nc, dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(dim, 2 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(2 * dim, 4 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.out_feat=out_feat\n",
        "        self.main = main\n",
        "        self.linear = nn.Linear(4*4*4*dim, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        output = output.view(-1, 4*4*4*self.dim)\n",
        "        if self.out_feat:\n",
        "            return output\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "     def __init__(self,dim, zdim, nc):\n",
        "         super(Encoder, self).__init__()\n",
        "         self.dim = dim\n",
        "         main = nn.Sequential(\n",
        "            nn.Conv2d(nc, dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(dim, 2 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(2 * dim, 4 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            )\n",
        "         self.main = main\n",
        "         self.linear = nn.Linear(4*4*4*dim, zdim)\n",
        "\n",
        "     def forward(self, input):\n",
        "         output = self.main(input)\n",
        "         output = output.view(-1, 4*4*4*self.dim)\n",
        "         output = self.linear(output)\n",
        "         return output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5voHg2ZOkpql"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbMKsARzhpw5"
      },
      "source": [
        "## Category \n",
        "arr = [\"XR_ELBOW\",\"XR_FINGER\",\"XR_FOREARM\",\"XR_HAND\",\"XR_HUMERUS\",\"XR_SHOULDER\",\"XR_WRIST\"]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0k4u1sjkpql"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw_3_j5jhw1H"
      },
      "source": [
        "## 1.XR_HUMERUS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em71jeGqleMF"
      },
      "source": [
        "# change the path here to shared drive\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)    #scs\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)\n",
        "\n",
        "\n",
        "mura_valid_df = customDf('/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)\n",
        "valid_dataset = MURA_dataset(mura_valid_df, '/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/', transforms)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW75BzS8lhRr"
      },
      "source": [
        "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfmnfrxhkpql"
      },
      "source": [
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
        "\n",
        "    Parameters:\n",
        "        input_image (tensor) --  the input image tensor array\n",
        "        imtype (type)        --  the desired type of the converted numpy array\n",
        "    \"\"\"\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor[0].cpu().float().numpy()  # convert it into a numpy array\n",
        "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
        "    else:  # if it is a numpy array, do nothing\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX88VDNQsg0o"
      },
      "source": [
        "### Metric Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPmVs1ckkpql",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "9b93ce43-c81a-4127-d11a-4b0ac4d8b407"
      },
      "source": [
        "\n",
        "\n",
        "n_epochs = 5001\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 128\n",
        "img_size = 64\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "abnormal_class = 0\n",
        "#device = 'cuda' \n",
        "out = '/content/drive/Shared drives/MeanSquare-Drive/RL-Project/AnoGAN/models/anoGAN-ckpts-XR_HUMERUS/' #scs- change here for all cat\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "max_auc = 0\n",
        "\n",
        "\n",
        "generator = Generator(dim = 64, zdim=latent_dim, nc=channels)\n",
        "discriminator = Discriminator(dim = 64, zdim=latent_dim, nc=channels,out_feat=True)\n",
        "encoder = Encoder(dim = 64, zdim=latent_dim, nc=channels)\n",
        "\n",
        "generator.load_state_dict(torch.load(out+'G_epoch5000.pt'))\n",
        "discriminator.load_state_dict(torch.load(out+'D_epoch5000.pt'))\n",
        "generator.to(device)\n",
        "encoder.to(device)\n",
        "discriminator.to(device)\n",
        "with torch.no_grad():\n",
        "    labels = torch.zeros(size=(len(valid_dataloader.dataset),),\n",
        "                                        dtype=torch.long, device=device)\n",
        "\n",
        "    scores = torch.empty(\n",
        "                size=(len(valid_dataloader.dataset),),\n",
        "                dtype=torch.float32,\n",
        "                device=device)\n",
        "    for i, (imgs, lbls) in enumerate(valid_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "\n",
        "            labels[i*batch_size:(i+1)*batch_size].copy_(lbls)\n",
        "            emb_query = encoder(imgs)\n",
        "            fake_imgs = generator(emb_query)\n",
        "            emb_fake = encoder(fake_imgs)\n",
        "\n",
        "            image_feats  = discriminator(imgs)\n",
        "            recon_feats = discriminator(fake_imgs)\n",
        "                \n",
        "            diff = imgs-fake_imgs\n",
        "            \n",
        "            image1_tensor= diff[0]\n",
        "           \n",
        "            im = tensor2im(imgs)\n",
        "            plt.imshow(im)\n",
        "            \n",
        "            im2 = tensor2im(fake_imgs)\n",
        "            plt.imshow(im2)\n",
        "            \n",
        "            im3 = tensor2im(diff)\n",
        "            plt.imshow(im3)\n",
        "            print(im.shape)\n",
        "            print(im3.shape)\n",
        "            #break   \n",
        "            \n",
        "            image_distance = torch.mean(torch.pow(imgs-fake_imgs, 2), dim=[1,2,3])\n",
        "            feat_distance = torch.mean(torch.pow(image_feats-recon_feats, 2), dim=1)\n",
        "\n",
        "            #z_distance = torch.nn.MSELoss(emb_query, emb_fake)#mse_loss(emb_query, emb_fake)\n",
        "            #print z_distance\n",
        "            #print('z_distance=',z_distance)\n",
        "            #print('hiiiiiiiii')\n",
        "            scores[i*batch_size:(i+1)*batch_size].copy_(feat_distance)\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    # scores = torch.mean(scores,)\n",
        "    scores = scores.cpu().squeeze()\n",
        "    print(scores.shape)\n",
        "\n",
        "    #print('\\n####################')\n",
        "    print('\\n######## Category: XR_HUMERUS #######')\n",
        "    \n",
        "    \n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('roc_auc=', roc_auc)\n",
        "    max_auc = max(roc_auc, max_auc)\n",
        "    print('max_auc=', max_auc)\n",
        "    \n",
        "    print(len(valid_dataloader.dataset))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "torch.Size([288])\n",
            "\n",
            "######## Category: XR_HUMERUS #######\n",
            "roc_auc= 0.4402992277992278\n",
            "max_auc= 0.4402992277992278\n",
            "288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am49dfzPkpqm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD8cusw50jT2"
      },
      "source": [
        "## 1.XR_ELBOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-pYUr580jT2"
      },
      "source": [
        "# change the path here to shared drive\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)    #scs\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)\n",
        "\n",
        "\n",
        "mura_valid_df = customDf('/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/MURA-v1.1/valid_image_paths.csv', 'XR_ELBOW', None)\n",
        "valid_dataset = MURA_dataset(mura_valid_df, '/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/', transforms)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYDPphdl0jT2"
      },
      "source": [
        "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFrXDIis0jT2"
      },
      "source": [
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
        "\n",
        "    Parameters:\n",
        "        input_image (tensor) --  the input image tensor array\n",
        "        imtype (type)        --  the desired type of the converted numpy array\n",
        "    \"\"\"\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor[0].cpu().float().numpy()  # convert it into a numpy array\n",
        "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
        "    else:  # if it is a numpy array, do nothing\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP_GgqJz0jT2"
      },
      "source": [
        "### Metric Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "deBLkQRM0jT2",
        "outputId": "4c7b05de-8978-4d85-a044-e935d5643760"
      },
      "source": [
        "n_epochs = 5001\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 128\n",
        "img_size = 64\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "abnormal_class = 0\n",
        "#device = 'cuda' \n",
        "out = '/content/drive/Shared drives/MeanSquare-Drive/RL-Project/AnoGAN/models/anoGAN-ckpts-XR_ELBOW/' #scs- change here for all cat\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "max_auc = 0\n",
        "\n",
        "\n",
        "generator = Generator(dim = 64, zdim=latent_dim, nc=channels)\n",
        "discriminator = Discriminator(dim = 64, zdim=latent_dim, nc=channels,out_feat=True)\n",
        "encoder = Encoder(dim = 64, zdim=latent_dim, nc=channels)\n",
        "\n",
        "generator.load_state_dict(torch.load(out+'G_epoch5000.pt'))\n",
        "discriminator.load_state_dict(torch.load(out+'D_epoch5000.pt'))\n",
        "generator.to(device)\n",
        "encoder.to(device)\n",
        "discriminator.to(device)\n",
        "with torch.no_grad():\n",
        "    labels = torch.zeros(size=(len(valid_dataloader.dataset),),\n",
        "                                        dtype=torch.long, device=device)\n",
        "\n",
        "    scores = torch.empty(\n",
        "                size=(len(valid_dataloader.dataset),),\n",
        "                dtype=torch.float32,\n",
        "                device=device)\n",
        "    for i, (imgs, lbls) in enumerate(valid_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "\n",
        "            labels[i*batch_size:(i+1)*batch_size].copy_(lbls)\n",
        "            emb_query = encoder(imgs)\n",
        "            fake_imgs = generator(emb_query)\n",
        "            emb_fake = encoder(fake_imgs)\n",
        "\n",
        "            image_feats  = discriminator(imgs)\n",
        "            recon_feats = discriminator(fake_imgs)\n",
        "                \n",
        "            diff = imgs-fake_imgs\n",
        "            \n",
        "            image1_tensor= diff[0]\n",
        "           \n",
        "            im = tensor2im(imgs)\n",
        "            plt.imshow(im)\n",
        "            \n",
        "            im2 = tensor2im(fake_imgs)\n",
        "            plt.imshow(im2)\n",
        "            \n",
        "            im3 = tensor2im(diff)\n",
        "            plt.imshow(im3)\n",
        "            print(im.shape)\n",
        "            print(im3.shape)\n",
        "            #break   \n",
        "            \n",
        "            image_distance = torch.mean(torch.pow(imgs-fake_imgs, 2), dim=[1,2,3])\n",
        "            feat_distance = torch.mean(torch.pow(image_feats-recon_feats, 2), dim=1)\n",
        "\n",
        "            #z_distance = torch.nn.MSELoss(emb_query, emb_fake)#mse_loss(emb_query, emb_fake)\n",
        "            #print z_distance\n",
        "            #print('z_distance=',z_distance)\n",
        "            #print('hiiiiiiiii')\n",
        "            scores[i*batch_size:(i+1)*batch_size].copy_(feat_distance)\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    # scores = torch.mean(scores,)\n",
        "    scores = scores.cpu().squeeze()\n",
        "    print(scores.shape)\n",
        "\n",
        "    #print('\\n####################')\n",
        "    print('\\n######## Category: XR_ELBOW #######')\n",
        "    \n",
        "    \n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('roc_auc=', roc_auc)\n",
        "    max_auc = max(roc_auc, max_auc)\n",
        "    print('max_auc=', max_auc)\n",
        "    \n",
        "    print(len(valid_dataloader.dataset))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "torch.Size([465])\n",
            "\n",
            "######## Category: XR_ELBOW #######\n",
            "roc_auc= 0.41851988899167447\n",
            "max_auc= 0.41851988899167447\n",
            "465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fa7Ve8G0jT2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}