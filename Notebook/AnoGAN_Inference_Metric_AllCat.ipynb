{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "AnoGAN_Inference_Metric_AllCat.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plodha/CMPE-297-DeepLearning/blob/main/Notebook/AnoGAN_Inference_Metric_AllCat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCzmSViMljTt"
      },
      "source": [
        "# Mount Drive and Set Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GImoCE9Uk0CF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8686bdda-a8bb-4617-9d54-63c71a500ed9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNhf6fvzk3Lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dde7395-21ac-48ab-d909-3ef46246a97b"
      },
      "source": [
        "!pip install torch==1.7.0 torchvision==0.5.0 tqdm opencv-python Pillow==8.0.1 tensorboardX==1.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (1.7.0)\n",
            "Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: Pillow==8.0.1 in /usr/local/lib/python3.6/dist-packages (8.0.1)\n",
            "Requirement already satisfied: tensorboardX==1.4 in /usr/local/lib/python3.6/dist-packages (1.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.4) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9XQVZ8yk5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0ed224-97c8-448d-966a-a8256a78e0b4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Dec  5 21:37:27 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    23W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvWbMV8Lk7mB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5755ec57-3f5e-4753-d527-6883852668ec"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  images  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktBKO9rWkpqi"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "import random\n",
        "import pandas as pd\n",
        "#import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "#from dataloader.dataloader import load_data\n",
        "#from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "#from networks import Generator, Discriminator\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "ngpu = 1\n",
        "os.makedirs(\"images\", exist_ok=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32g4Nghbkpqj"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "class MURA_dataset(Dataset):\n",
        "    '''\n",
        "    Dataset class for MURA dataset\n",
        "    Args:\n",
        "        - df: Dataframe with the first columns contains the path to the images\n",
        "        - root_dir: string contains path of  root directory\n",
        "        - transforms: Pytorch transform operations\n",
        "    '''\n",
        "\n",
        "    def __init__(self, df, root_dir, transforms=None):\n",
        "        #print(\"I am calling Mura dataset\")\n",
        "        self.df = df\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.df.iloc[idx, 0])\n",
        "        #print('img_name ',img_name)\n",
        "        img = cv2.imread(img_name)\n",
        "        #print('img shape ',img.shape)\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        if 'negative' in img_name: label = 0\n",
        "        else: label = 1\n",
        "\n",
        "        return img, label"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hqNZtKDkpqj"
      },
      "source": [
        "def transform(rotation, hflip, resize, totensor, normalize, centercrop, to_pil, gray):\n",
        "    options = []\n",
        "    if to_pil:\n",
        "        options.append(torchvision.transforms.ToPILImage())\n",
        "    if gray:\n",
        "        options.append(torchvision.transforms.Grayscale())\n",
        "    if rotation:\n",
        "        options.append(torchvision.transforms.RandomRotation(20))\n",
        "    if hflip:\n",
        "        options.append(torchvision.transforms.RandomHorizontalFlip())\n",
        "    if centercrop:\n",
        "        options.append(torchvision.transforms.CenterCrop(256))\n",
        "    if resize:\n",
        "        options.append(torchvision.transforms.Resize((32,32)))\n",
        "    if totensor:\n",
        "        options.append(torchvision.transforms.ToTensor())\n",
        "    # if True:\n",
        "    #     options.append(transforms.Lambda(lambda x: (x - x.min())/(x.max()-x.min())))\n",
        "    if normalize:\n",
        "        options.append(torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
        "    transform = torchvision.transforms.Compose(options)\n",
        "    return transform"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar_SQ2hhkpqj"
      },
      "source": [
        "def customDf(path, studyClass=None, studyType=None):\n",
        "    '''\n",
        "    Function to get custom csv based on class of study and type of study\n",
        "    Args:\n",
        "        - path(string): path to original csv\n",
        "        - studyClass(list): class of study, list must contains one of the following:\n",
        "            \"XR_ELBOW\",\n",
        "            \"XR_FINGER\",\n",
        "            \"XR_FOREARM\",\n",
        "            \"XR_HAND\",\n",
        "            \"XR_HUMERUS\",\n",
        "            \"XR_SHOULDER\",\n",
        "            \"XR_WRIST\"\n",
        "            if None, take all\n",
        "        - studyResult(list): Result of study, list must contains one of the following:\n",
        "            \"positive\", \"negative\"\n",
        "            if None, take all\n",
        "    '''\n",
        "    df = pd.read_csv(path, header=None)\n",
        "\n",
        "    if studyClass:\n",
        "        cond = df[0].str.contains(studyClass)\n",
        "        df = df[cond]\n",
        "    if studyType:\n",
        "        cond = df[0].str.contains(studyType)\n",
        "        df = df[cond]\n",
        "    return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-A8jeDGkpqj"
      },
      "source": [
        "#import pandas as pd\n",
        "#import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyMShdYgkpqj"
      },
      "source": [
        "#valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MZ4oVUFkpqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0fbb484-b2b0-49d0-a49e-4f78a11a0f48"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGrV7RkUkpql"
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdSxrnohkpql"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umqs0EXGkpql"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, dim, zdim, nc):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.dim = dim\n",
        "        preprocess = nn.Sequential(\n",
        "            nn.Linear(zdim, 4 * 4 * 4 * dim),\n",
        "            nn.BatchNorm1d(4 * 4 * 4 * dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        block1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(4 * dim, 2 * dim, 2, stride=2),\n",
        "            nn.BatchNorm2d(2 * dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        block2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2 * dim, dim, 2, stride=2),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        deconv_out = nn.ConvTranspose2d(dim, nc, 2, stride=2)\n",
        "\n",
        "        self.preprocess = preprocess\n",
        "        self.block1 = block1\n",
        "        self.block2 = block2\n",
        "        self.deconv_out = deconv_out\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.preprocess(input)\n",
        "        output = output.view(-1, 4 * self.dim, 4, 4)\n",
        "        output = self.block1(output)\n",
        "        output = self.block2(output)\n",
        "        output = self.deconv_out(output)\n",
        "        output = self.tanh(output)\n",
        "        return output.view(-1, self.nc, 32, 32)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAqyGQqQkpql"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qfMbK1skpql"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, dim, zdim, nc, out_feat=False):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.dim = dim\n",
        "        main = nn.Sequential(\n",
        "            nn.Conv2d(nc, dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(dim, 2 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(2 * dim, 4 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.out_feat=out_feat\n",
        "        self.main = main\n",
        "        self.linear = nn.Linear(4*4*4*dim, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        output = output.view(-1, 4*4*4*self.dim)\n",
        "        if self.out_feat:\n",
        "            return output\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "     def __init__(self,dim, zdim, nc):\n",
        "         super(Encoder, self).__init__()\n",
        "         self.dim = dim\n",
        "         main = nn.Sequential(\n",
        "            nn.Conv2d(nc, dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(dim, 2 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(2 * dim, 4 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            )\n",
        "         self.main = main\n",
        "         self.linear = nn.Linear(4*4*4*dim, zdim)\n",
        "\n",
        "     def forward(self, input):\n",
        "         output = self.main(input)\n",
        "         output = output.view(-1, 4*4*4*self.dim)\n",
        "         output = self.linear(output)\n",
        "         return output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5voHg2ZOkpql"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbMKsARzhpw5"
      },
      "source": [
        "## Category \n",
        "arr = [\"XR_ELBOW\",\"XR_FINGER\",\"XR_FOREARM\",\"XR_HAND\",\"XR_HUMERUS\",\"XR_SHOULDER\",\"XR_WRIST\"]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0k4u1sjkpql"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw_3_j5jhw1H"
      },
      "source": [
        "## 1.XR_HUMERUS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em71jeGqleMF"
      },
      "source": [
        "# change the path here to shared drive\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)    #scs\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)\n",
        "\n",
        "\n",
        "mura_valid_df = customDf('/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)\n",
        "valid_dataset = MURA_dataset(mura_valid_df, '/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/', transforms)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW75BzS8lhRr"
      },
      "source": [
        "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfmnfrxhkpql"
      },
      "source": [
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
        "\n",
        "    Parameters:\n",
        "        input_image (tensor) --  the input image tensor array\n",
        "        imtype (type)        --  the desired type of the converted numpy array\n",
        "    \"\"\"\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor[0].cpu().float().numpy()  # convert it into a numpy array\n",
        "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
        "    else:  # if it is a numpy array, do nothing\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX88VDNQsg0o"
      },
      "source": [
        "### Metric Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL0zW51a2ekm"
      },
      "source": [
        "from torch.nn.functional import mse_loss"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPmVs1ckkpql",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "161c0da7-68bb-45d5-e3a9-9782816de50e"
      },
      "source": [
        "\n",
        "\n",
        "n_epochs = 5001\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 128\n",
        "img_size = 64\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "abnormal_class = 0\n",
        "#device = 'cuda' \n",
        "out = '/content/drive/Shared drives/MeanSquare-Drive/RL-Project/AnoGAN/models/anoGAN-ckpts-XR_HUMERUS/' #scs- change here for all cat\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "max_auc = 0\n",
        "\n",
        "\n",
        "generator = Generator(dim = 64, zdim=latent_dim, nc=channels)\n",
        "discriminator = Discriminator(dim = 64, zdim=latent_dim, nc=channels,out_feat=True)\n",
        "encoder = Encoder(dim = 64, zdim=latent_dim, nc=channels)\n",
        "\n",
        "generator.load_state_dict(torch.load(out+'G_epoch5000.pt'))\n",
        "discriminator.load_state_dict(torch.load(out+'D_epoch5000.pt'))\n",
        "generator.to(device)\n",
        "encoder.to(device)\n",
        "discriminator.to(device)\n",
        "with torch.no_grad():\n",
        "    labels = torch.zeros(size=(len(valid_dataloader.dataset),),\n",
        "                                        dtype=torch.long, device=device)\n",
        "\n",
        "    scores = torch.empty(\n",
        "                size=(len(valid_dataloader.dataset),),\n",
        "                dtype=torch.float32,\n",
        "                device=device)\n",
        "    for i, (imgs, lbls) in enumerate(valid_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "\n",
        "            labels[i*batch_size:(i+1)*batch_size].copy_(lbls)\n",
        "            emb_query = encoder(imgs)\n",
        "            fake_imgs = generator(emb_query)\n",
        "            emb_fake = encoder(fake_imgs)\n",
        "\n",
        "            image_feats  = discriminator(imgs)\n",
        "            recon_feats = discriminator(fake_imgs)\n",
        "                \n",
        "            diff = imgs-fake_imgs\n",
        "            \n",
        "            image1_tensor= diff[0]\n",
        "           \n",
        "            im = tensor2im(imgs)\n",
        "            plt.imshow(im)\n",
        "            \n",
        "            im2 = tensor2im(fake_imgs)\n",
        "            plt.imshow(im2)\n",
        "            \n",
        "            im3 = tensor2im(diff)\n",
        "            plt.imshow(im3)\n",
        "            print(im.shape)\n",
        "            print(im3.shape)\n",
        "            #break   \n",
        "            \n",
        "            image_distance = torch.mean(torch.pow(imgs-fake_imgs, 2), dim=[1,2,3])\n",
        "            feat_distance = torch.mean(torch.pow(image_feats-recon_feats, 2), dim=1)\n",
        "            print(emb_query.shape, emb_fake.shape)\n",
        "            z_distance = mse_loss(emb_query, emb_fake)#mse_loss(emb_query, emb_fake)\n",
        "            #print z_distance\n",
        "            print('z_distance=',z_distance)\n",
        "            #print('hiiiiiiiii')\n",
        "            scores[i*batch_size:(i+1)*batch_size].copy_(feat_distance)\n",
        "            break\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    # scores = torch.mean(scores,)\n",
        "    scores = scores.cpu().squeeze()\n",
        "    print(scores.shape)\n",
        "\n",
        "    #print('\\n####################')\n",
        "    print('\\n######## Category: XR_HUMERUS #######')\n",
        "    \n",
        "    \n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('roc_auc=', roc_auc)\n",
        "    max_auc = max(roc_auc, max_auc)\n",
        "    print('max_auc=', max_auc)\n",
        "    \n",
        "    print(len(valid_dataloader.dataset))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "torch.Size([64, 128]) torch.Size([64, 128])\n",
            "z_distance= tensor(8.5171e-05, device='cuda:0')\n",
            "torch.Size([288])\n",
            "\n",
            "######## Category: XR_HUMERUS #######\n",
            "roc_auc= 0.9395127748068924\n",
            "max_auc= 0.9395127748068924\n",
            "288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am49dfzPkpqm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD8cusw50jT2"
      },
      "source": [
        "## 1.XR_ELBOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-pYUr580jT2"
      },
      "source": [
        "# change the path here to shared drive\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)    #scs\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)\n",
        "\n",
        "\n",
        "mura_valid_df = customDf('/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/MURA-v1.1/valid_image_paths.csv', 'XR_ELBOW', None)\n",
        "valid_dataset = MURA_dataset(mura_valid_df, '/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/', transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYDPphdl0jT2"
      },
      "source": [
        "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFrXDIis0jT2"
      },
      "source": [
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
        "\n",
        "    Parameters:\n",
        "        input_image (tensor) --  the input image tensor array\n",
        "        imtype (type)        --  the desired type of the converted numpy array\n",
        "    \"\"\"\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor[0].cpu().float().numpy()  # convert it into a numpy array\n",
        "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
        "    else:  # if it is a numpy array, do nothing\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP_GgqJz0jT2"
      },
      "source": [
        "### Metric Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "deBLkQRM0jT2",
        "outputId": "4c7b05de-8978-4d85-a044-e935d5643760"
      },
      "source": [
        "n_epochs = 5001\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 128\n",
        "img_size = 64\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "abnormal_class = 0\n",
        "#device = 'cuda' \n",
        "out = '/content/drive/Shared drives/MeanSquare-Drive/RL-Project/AnoGAN/models/anoGAN-ckpts-XR_ELBOW/' #scs- change here for all cat\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "max_auc = 0\n",
        "\n",
        "\n",
        "generator = Generator(dim = 64, zdim=latent_dim, nc=channels)\n",
        "discriminator = Discriminator(dim = 64, zdim=latent_dim, nc=channels,out_feat=True)\n",
        "encoder = Encoder(dim = 64, zdim=latent_dim, nc=channels)\n",
        "\n",
        "generator.load_state_dict(torch.load(out+'G_epoch5000.pt'))\n",
        "discriminator.load_state_dict(torch.load(out+'D_epoch5000.pt'))\n",
        "generator.to(device)\n",
        "encoder.to(device)\n",
        "discriminator.to(device)\n",
        "with torch.no_grad():\n",
        "    labels = torch.zeros(size=(len(valid_dataloader.dataset),),\n",
        "                                        dtype=torch.long, device=device)\n",
        "\n",
        "    scores = torch.empty(\n",
        "                size=(len(valid_dataloader.dataset),),\n",
        "                dtype=torch.float32,\n",
        "                device=device)\n",
        "    for i, (imgs, lbls) in enumerate(valid_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "\n",
        "            labels[i*batch_size:(i+1)*batch_size].copy_(lbls)\n",
        "            emb_query = encoder(imgs)\n",
        "            fake_imgs = generator(emb_query)\n",
        "            emb_fake = encoder(fake_imgs)\n",
        "\n",
        "            image_feats  = discriminator(imgs)\n",
        "            recon_feats = discriminator(fake_imgs)\n",
        "                \n",
        "            diff = imgs-fake_imgs\n",
        "            \n",
        "            image1_tensor= diff[0]\n",
        "           \n",
        "            im = tensor2im(imgs)\n",
        "            plt.imshow(im)\n",
        "            \n",
        "            im2 = tensor2im(fake_imgs)\n",
        "            plt.imshow(im2)\n",
        "            \n",
        "            im3 = tensor2im(diff)\n",
        "            plt.imshow(im3)\n",
        "            print(im.shape)\n",
        "            print(im3.shape)\n",
        "            #break   \n",
        "            \n",
        "            image_distance = torch.mean(torch.pow(imgs-fake_imgs, 2), dim=[1,2,3])\n",
        "            feat_distance = torch.mean(torch.pow(image_feats-recon_feats, 2), dim=1)\n",
        "\n",
        "            #z_distance = torch.nn.MSELoss(emb_query, emb_fake)#mse_loss(emb_query, emb_fake)\n",
        "            #print z_distance\n",
        "            #print('z_distance=',z_distance)\n",
        "            #print('hiiiiiiiii')\n",
        "            scores[i*batch_size:(i+1)*batch_size].copy_(feat_distance)\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    # scores = torch.mean(scores,)\n",
        "    scores = scores.cpu().squeeze()\n",
        "    print(scores.shape)\n",
        "\n",
        "    #print('\\n####################')\n",
        "    print('\\n######## Category: XR_ELBOW #######')\n",
        "    \n",
        "    \n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('roc_auc=', roc_auc)\n",
        "    max_auc = max(roc_auc, max_auc)\n",
        "    print('max_auc=', max_auc)\n",
        "    \n",
        "    print(len(valid_dataloader.dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "torch.Size([465])\n",
            "\n",
            "######## Category: XR_ELBOW #######\n",
            "roc_auc= 0.41851988899167447\n",
            "max_auc= 0.41851988899167447\n",
            "465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fa7Ve8G0jT2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}