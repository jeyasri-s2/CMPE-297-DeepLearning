{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "AnoGAN_Inference_Metric_AllCat.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plodha/CMPE-297-DeepLearning/blob/main/Notebook/AnoGAN_Inference_Metric_AllCat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCzmSViMljTt"
      },
      "source": [
        "# Mount Drive and Set Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GImoCE9Uk0CF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb7484e-39a9-4553-bf2d-59146c5008d7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNhf6fvzk3Lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dde7395-21ac-48ab-d909-3ef46246a97b"
      },
      "source": [
        "!pip install torch==1.7.0 torchvision==0.5.0 tqdm opencv-python Pillow==8.0.1 tensorboardX==1.4"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (1.7.0)\n",
            "Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: Pillow==8.0.1 in /usr/local/lib/python3.6/dist-packages (8.0.1)\n",
            "Requirement already satisfied: tensorboardX==1.4 in /usr/local/lib/python3.6/dist-packages (1.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.4) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9XQVZ8yk5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87019d6a-5b22-4384-ed89-3e4e0bbc44cf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Dec  5 20:59:46 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvWbMV8Lk7mB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5755ec57-3f5e-4753-d527-6883852668ec"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  images  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktBKO9rWkpqi"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "import random\n",
        "import pandas as pd\n",
        "#import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "#from dataloader.dataloader import load_data\n",
        "#from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "#from networks import Generator, Discriminator\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "ngpu = 1\n",
        "os.makedirs(\"images\", exist_ok=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32g4Nghbkpqj"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "class MURA_dataset(Dataset):\n",
        "    '''\n",
        "    Dataset class for MURA dataset\n",
        "    Args:\n",
        "        - df: Dataframe with the first columns contains the path to the images\n",
        "        - root_dir: string contains path of  root directory\n",
        "        - transforms: Pytorch transform operations\n",
        "    '''\n",
        "\n",
        "    def __init__(self, df, root_dir, transforms=None):\n",
        "        #print(\"I am calling Mura dataset\")\n",
        "        self.df = df\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.df.iloc[idx, 0])\n",
        "        #print('img_name ',img_name)\n",
        "        img = cv2.imread(img_name)\n",
        "        #print('img shape ',img.shape)\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        if 'negative' in img_name: label = 0\n",
        "        else: label = 1\n",
        "\n",
        "        return img, label"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hqNZtKDkpqj"
      },
      "source": [
        "def transform(rotation, hflip, resize, totensor, normalize, centercrop, to_pil, gray):\n",
        "    options = []\n",
        "    if to_pil:\n",
        "        options.append(torchvision.transforms.ToPILImage())\n",
        "    if gray:\n",
        "        options.append(torchvision.transforms.Grayscale())\n",
        "    if rotation:\n",
        "        options.append(torchvision.transforms.RandomRotation(20))\n",
        "    if hflip:\n",
        "        options.append(torchvision.transforms.RandomHorizontalFlip())\n",
        "    if centercrop:\n",
        "        options.append(torchvision.transforms.CenterCrop(256))\n",
        "    if resize:\n",
        "        options.append(torchvision.transforms.Resize((32,32)))\n",
        "    if totensor:\n",
        "        options.append(torchvision.transforms.ToTensor())\n",
        "    # if True:\n",
        "    #     options.append(transforms.Lambda(lambda x: (x - x.min())/(x.max()-x.min())))\n",
        "    if normalize:\n",
        "        options.append(torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
        "    transform = torchvision.transforms.Compose(options)\n",
        "    return transform"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar_SQ2hhkpqj"
      },
      "source": [
        "def customDf(path, studyClass=None, studyType=None):\n",
        "    '''\n",
        "    Function to get custom csv based on class of study and type of study\n",
        "    Args:\n",
        "        - path(string): path to original csv\n",
        "        - studyClass(list): class of study, list must contains one of the following:\n",
        "            \"XR_ELBOW\",\n",
        "            \"XR_FINGER\",\n",
        "            \"XR_FOREARM\",\n",
        "            \"XR_HAND\",\n",
        "            \"XR_HUMERUS\",\n",
        "            \"XR_SHOULDER\",\n",
        "            \"XR_WRIST\"\n",
        "            if None, take all\n",
        "        - studyResult(list): Result of study, list must contains one of the following:\n",
        "            \"positive\", \"negative\"\n",
        "            if None, take all\n",
        "    '''\n",
        "    df = pd.read_csv(path, header=None)\n",
        "\n",
        "    if studyClass:\n",
        "        cond = df[0].str.contains(studyClass)\n",
        "        df = df[cond]\n",
        "    if studyType:\n",
        "        cond = df[0].str.contains(studyType)\n",
        "        df = df[cond]\n",
        "    return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-A8jeDGkpqj"
      },
      "source": [
        "#import pandas as pd\n",
        "#import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyMShdYgkpqj"
      },
      "source": [
        "#valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MZ4oVUFkpqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31716082-6ff4-4ce8-df8c-f0cd268b71c7"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGrV7RkUkpql"
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdSxrnohkpql"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umqs0EXGkpql"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, dim, zdim, nc):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.dim = dim\n",
        "        preprocess = nn.Sequential(\n",
        "            nn.Linear(zdim, 4 * 4 * 4 * dim),\n",
        "            nn.BatchNorm1d(4 * 4 * 4 * dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        block1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(4 * dim, 2 * dim, 2, stride=2),\n",
        "            nn.BatchNorm2d(2 * dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        block2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2 * dim, dim, 2, stride=2),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        deconv_out = nn.ConvTranspose2d(dim, nc, 2, stride=2)\n",
        "\n",
        "        self.preprocess = preprocess\n",
        "        self.block1 = block1\n",
        "        self.block2 = block2\n",
        "        self.deconv_out = deconv_out\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.preprocess(input)\n",
        "        output = output.view(-1, 4 * self.dim, 4, 4)\n",
        "        output = self.block1(output)\n",
        "        output = self.block2(output)\n",
        "        output = self.deconv_out(output)\n",
        "        output = self.tanh(output)\n",
        "        return output.view(-1, self.nc, 32, 32)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAqyGQqQkpql"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qfMbK1skpql"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, dim, zdim, nc, out_feat=False):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.dim = dim\n",
        "        main = nn.Sequential(\n",
        "            nn.Conv2d(nc, dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(dim, 2 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(2 * dim, 4 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.out_feat=out_feat\n",
        "        self.main = main\n",
        "        self.linear = nn.Linear(4*4*4*dim, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        output = output.view(-1, 4*4*4*self.dim)\n",
        "        if self.out_feat:\n",
        "            return output\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "     def __init__(self,dim, zdim, nc):\n",
        "         super(Encoder, self).__init__()\n",
        "         self.dim = dim\n",
        "         main = nn.Sequential(\n",
        "            nn.Conv2d(nc, dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(dim, 2 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(2 * dim, 4 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            )\n",
        "         self.main = main\n",
        "         self.linear = nn.Linear(4*4*4*dim, zdim)\n",
        "\n",
        "     def forward(self, input):\n",
        "         output = self.main(input)\n",
        "         output = output.view(-1, 4*4*4*self.dim)\n",
        "         output = self.linear(output)\n",
        "         return output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5voHg2ZOkpql"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbMKsARzhpw5"
      },
      "source": [
        "## Category \n",
        "arr = [\"XR_ELBOW\",\"XR_FINGER\",\"XR_FOREARM\",\"XR_HAND\",\"XR_HUMERUS\",\"XR_SHOULDER\",\"XR_WRIST\"]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0k4u1sjkpql"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw_3_j5jhw1H"
      },
      "source": [
        "## 1.XR_HUMERUS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em71jeGqleMF"
      },
      "source": [
        "# change the path here to shared drive\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)    #scs\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)\n",
        "\n",
        "\n",
        "mura_valid_df = customDf('/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)\n",
        "valid_dataset = MURA_dataset(mura_valid_df, '/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/', transforms)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW75BzS8lhRr"
      },
      "source": [
        "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfmnfrxhkpql"
      },
      "source": [
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
        "\n",
        "    Parameters:\n",
        "        input_image (tensor) --  the input image tensor array\n",
        "        imtype (type)        --  the desired type of the converted numpy array\n",
        "    \"\"\"\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor[0].cpu().float().numpy()  # convert it into a numpy array\n",
        "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
        "    else:  # if it is a numpy array, do nothing\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX88VDNQsg0o"
      },
      "source": [
        "### Metric Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPmVs1ckkpql",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "b67e945d-9102-4373-bccf-acd163bc0c0a"
      },
      "source": [
        "\n",
        "\n",
        "n_epochs = 5001\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 128\n",
        "img_size = 64\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "abnormal_class = 0\n",
        "#device = 'cuda' \n",
        "out = '/content/drive/Shared drives/MeanSquare-Drive/RL-Project/AnoGAN/models/anoGAN-ckpts-XR_HUMERUS/' #scs- change here for all cat\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "max_auc = 0\n",
        "\n",
        "\n",
        "generator = Generator(dim = 64, zdim=latent_dim, nc=channels)\n",
        "discriminator = Discriminator(dim = 64, zdim=latent_dim, nc=channels,out_feat=True)\n",
        "encoder = Encoder(dim = 64, zdim=latent_dim, nc=channels)\n",
        "\n",
        "generator.load_state_dict(torch.load(out+'G_epoch5000.pt'))\n",
        "discriminator.load_state_dict(torch.load(out+'D_epoch5000.pt'))\n",
        "generator.to(device)\n",
        "encoder.to(device)\n",
        "discriminator.to(device)\n",
        "with torch.no_grad():\n",
        "    labels = torch.zeros(size=(len(valid_dataloader.dataset),),\n",
        "                                        dtype=torch.long, device=device)\n",
        "\n",
        "    scores = torch.empty(\n",
        "                size=(len(valid_dataloader.dataset),),\n",
        "                dtype=torch.float32,\n",
        "                device=device)\n",
        "    for i, (imgs, lbls) in enumerate(valid_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "\n",
        "            labels[i*batch_size:(i+1)*batch_size].copy_(lbls)\n",
        "            emb_query = encoder(imgs)\n",
        "            fake_imgs = generator(emb_query)\n",
        "\n",
        "            image_feats  = discriminator(imgs)\n",
        "            recon_feats = discriminator(fake_imgs)\n",
        "                \n",
        "            diff = imgs-fake_imgs\n",
        "            \n",
        "            image1_tensor= diff[0]\n",
        "           \n",
        "            im = tensor2im(imgs)\n",
        "            plt.imshow(im)\n",
        "            \n",
        "            im2 = tensor2im(fake_imgs)\n",
        "            plt.imshow(im2)\n",
        "            \n",
        "            im3 = tensor2im(diff)\n",
        "            plt.imshow(im3)\n",
        "            print(im.shape)\n",
        "            print(im3.shape)\n",
        "            break   \n",
        "            \n",
        "            image_distance = torch.mean(torch.pow(imgs-fake_imgs, 2), dim=[1,2,3])\n",
        "            feat_distance = torch.mean(torch.pow(image_feats-recon_feats, 2), dim=1)\n",
        "\n",
        "            # z_distance = mse_loss(emb_query, emb_fake)\n",
        "            scores[i*batch_size:(i+1)*batch_size].copy_(feat_distance)\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    # scores = torch.mean(scores,)\n",
        "    scores = scores.cpu().squeeze()\n",
        "    print(scores.shape)\n",
        "\n",
        "    print('\\n####################')\n",
        "    print('Category: XR_HUMERUS')\n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('roc_auc=', roc_auc)\n",
        "    max_auc = max(roc_auc, max_auc)\n",
        "    print('max_auc=', max_auc)\n",
        "    \n",
        "    print(len(valid_dataloader.dataset))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "torch.Size([288])\n",
            "\n",
            "####################\n",
            "Category: XR_HUMERUS\n",
            "roc_auc= 0.35448232323232326\n",
            "max_auc= 0.35448232323232326\n",
            "288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbSklEQVR4nO2dW4xkV3WG/1WnLl19mRnPTM/dioFYihwrGNSyiECIgEAOimSQwMAD8oPFoAhLQSIPliMFR8oDRAHEQ0Q0xBYmIhgHG2GQleBYEIsXw5jYY4O52I4HZpiL59LTPd1dXZez8lDHoW2df/W1qsbs/5NGU7137XN27TqrLvuvfy1zdwghfv+pjHoCQojhoGAXIhEU7EIkgoJdiERQsAuRCAp2IRKhupnBZnYTgC8CyAD8i7t/Jrp/s9n0qampjZyntL1S4a9VWZat+zwAEEmR0fk2Qnu5zecBPg+2Hq8FornneU77ut3uho5ZrZZf4s1mk47ZuXMnn0eHz2P20izti66rrFJ+rbaWW3RML++Vts/Pz6O11CpdkA0Hu5llAP4JwLsBnADwYzN7yN1/xsZMTU3hgx/4YGlfDv5Ej9Uape0TU5N0zOQE74teCNrLy7SvMT5W2l4x/iKQ5/xJ/vXx47QvCvZo/hWUX/h5cLyQaFjw2lchAWjBWi0uLtG+8xfO076xRvn1AQC7du8ubb/++j+mYz70wQ/RvnPnztG+73z3O7Rvuc1f2LdPbitt//kvf0HHXF6cL21/8IEH6ZjNvFXdCOA5d3/B3dsA7gNw8yaOJ4QYIJsJ9oMAfrPi7xNFmxDiCmTgG3RmdtjMjprZ0aUl/jFNCDFYNhPsJwFcveLvQ0XbK3D3I+4+4+4z0aaIEGKwbCbYfwzgWjN7nZnVAXwYwENbMy0hxFaz4d14d++a2e0A/hN96e0ed//pqgPJy0sleN1hG9qtFpcmok8RufOdf8sCWYsMq43V6JBanfdNbS/fhQWA+bk5Po1AxmHzN7JLDwAeKAYR0THBduMD+XKsyXfV67U67ev2uBzWJrvgl2Yv0TFPHXuK9p09c5b2dTod2pcH0mE3L+/bMz3Nz3WiXDViagywSZ3d3R8G8PBmjiGEGA76BZ0QiaBgFyIRFOxCJIKCXYhEULALkQib2o1fL2aGaqX8lJERppuXSxreKnf+AMDcfLlRAAC2TXHJqxpIZb1e+fnyHp975GzbvWsX7VtaXKR9ocTDHIJ0BGCB9FYhrrFVD0oO6cFaZeTaAIDGGJflllv8l5nLxDl24Rw31vzgv39A+wLVFh50ZjX+2OYvXy5t3x5Is+fOla9HJG3qnV2IRFCwC5EICnYhEkHBLkQiKNiFSISh7sYDAPudfpTaychrUqfDU0hVgzRAkSmhMTFB+1Ar32JeWFigQ+rB7n600x3lQYvMGHS3O0hllVcCY02QF44+meD5+iLVJQtyyTWD1FPM7AJwNWQpSD9WqQVpv4z3ZRm/hiMVYrlbPhcf56nVpolJhuXcA/TOLkQyKNiFSAQFuxCJoGAXIhEU7EIkgoJdiEQYuhGmVi2XoqKKRp12ufEjC2SGVptLK7NBfrdanec6q9XLz9cIZKGoZFQ14/Pfu3cv7bs0x/OnLbfKpaYw715Ukil4XiKpjFXCiavn8Dk2A0m00+WGqBaR2KL1YNcoEJdxigxRVll/DsDLLS7pTk/vKW2X9CaEULALkQoKdiESQcEuRCIo2IVIBAW7EImwKenNzF4EMA+gB6Dr7jPR/R1AbkSSCSoQsbxaWZA7zXMux7Q73CV1Kchdt2d6d2l7JXA7MdkQAJrVMdrXC5x5UVmgEyd/W9oeSUaVSEKLpKZAvqoyd1jw9hLNMZIH6w0ulbVIfrpO4JTrBmtfrfFzwfk1Fz02lruOzR0AesH1zdgKnf3P3P3cFhxHCDFA9DFeiETYbLA7gO+Z2RNmdngrJiSEGAyb/Rj/Nnc/aWZ7ADxiZj9398dW3qF4ETgMAFNTU5s8nRBio2zqnd3dTxb/nwXwLQA3ltzniLvPuPtMc3x8M6cTQmyCDQe7mU2Y2dTLtwG8B8AzWzUxIcTWspmP8XsBfKuQRKoA/s3d/yMaYAAqJElh5HhyJtcFDqoKGdOfCJdxlpZ42SVWUuqqHTvomMhB1e5yWc6CZI5jDS7Z7di+vbR9dpY75YKVQiVwa7Hnst9J+kJ5LUjYGMh89WA9rFLuHIseM3OhAUAvKL3lwVpFiTtz4tqL5DrmfGQlyoBNBLu7vwDgjRsdL4QYLpLehEgEBbsQiaBgFyIRFOxCJIKCXYhEGHqtN+peClQcOCsQFyT4C0qURZJdpMlcJokqJ4IfC401uSzEkjICQK3Gn5pIljt46GBpe6vVomMil1fkeqsE9eNYos1ITuoFiSOjOmq1Cp9Hg9Ta63W56y26FpkDczWiWm85u+gCGW1xsfz5jCRsvbMLkQgKdiESQcEuRCIo2IVIBAW7EIkw9N14tvEYlXICMR948FrVaPKSTIhy1wWGCydKwsULF+mY3UG+uOVlnmOsMjFJ+8bGmrSPceDAAdp36renaF83yKEXGYpYT5RLLmvw0lvRuKjE1rZt5cagublZOqa1xJ+X8XFehsoDKSc0GxG1iYlQ/c7oiOQ86x4hhHhNomAXIhEU7EIkgoJdiERQsAuRCAp2IRJh+NIbkxMCyYvJcr1eYOAITASRgcOC1z9moGktc5PJ3DzP/bZv337al+f8sXWDx82kw6t2XkXHzJF8ZgAwe4FLVBYISsbMKYGclPf48apV/rxEshwzIi0u8Eu/ExiDLMgzF+Wu8zDHIusIykkhKENF0Du7EImgYBciERTsQiSCgl2IRFCwC5EICnYhEmFV6c3M7gHwFwDOuvv1RdtOAN8AcA2AFwHc4u7c+vW7Y6GalZ+S5uFCIFsExp/IE5RHMl+Qz4wpJFGJp6VF7qBaWLhM+6pRXrUx7ugba5Q74mpk3QFg3959tG9+rrx8EgAgKLFFBarAypVlkc0rcNiF7rvyvrEmdw5G7rUoX180LrquqNQXSXlEWo5y/K3lnf0rAG56VdsdAB5192sBPFr8LYS4glk12It66xde1XwzgHuL2/cCeN8Wz0sIscVs9Dv7Xnd/OePBafQrugohrmA2vUHn/S8J9IuCmR02s6NmdnRxkZdDFkIMlo0G+xkz2w8Axf9n2R3d/Yi7z7j7zHhQTEEIMVg2GuwPAbi1uH0rgG9vzXSEEINiLdLb1wG8A8BuMzsB4NMAPgPgfjO7DcBxALdsdiKVQD5h3p8o0WDUF5UZslpQ0oiUIIrKMXkgT81e5I6yQ6SME8DlNQBot5dL2wMVhz4uAJjevYv2nTt/jh+UPJ+RNJQZX/s8cIBlGZc+WTmkSHrrdDeWZDMLyopFUjCTlsOSURtIOLlqsLv7R0jXu9Z9NiHEyNAv6IRIBAW7EImgYBciERTsQiSCgl2IRBh6wsmsWi6vMIkE4HKNBVIHPxrQC2QLy7nEU62VL1ckvUWSUYfUsAOAC0Gix337uOuNJXqM/GRLrXK5DgB27NpJ+xaW+C8iF4nbrxok+4xkuWo1qgPHr4MqkVI9WJEsTEjKieYfQg4aJbdUrTchBEXBLkQiKNiFSAQFuxCJoGAXIhEU7EIkwtClt0hiY1QrLEklP1YvkNAC4xLywB7G5h5Jbxt15i0H9eOiRJVT27aXtkcyXzNwgEVz3LtnD+07ceJEaftG3FoAwuSWkbxZ9fJrh9XEA4CxBpc2I3mtF9QXDGsPErefV4LjhT46cp51jxBCvCZRsAuRCAp2IRJBwS5EIijYhUiEoe/G01I90SY9mSXbaQXindEo59pGduMrQdmiLFjieoObOxCYIKLcdfXGWGl7ZEDpBQpJu8XLV9VqfP47duwobT9//tX1Rn5HpRrkcAvmmAfySpc9Z8H7HNsdB4Cszs+1uMhLQ0XPZ85yIgYb7uz63mz5JyHE7wEKdiESQcEuRCIo2IVIBAW7EImgYBciEdZS/ukeAH8B4Ky7X1+03QXgYwBeKu52p7s/vKmZBPJVjeRVi0wVkTklyjGWV8JCPeRcgVRD8tYBQBaYTKhECWCZlHgCgHMvvVTavmd6ms8jWo+gj+XkA4D9Bw6Uts8HJp5Oi5t1KnV+LgvkUsvK+9wCrTd4Cxxv8uKkraU27ev1uCzHrtVKINdFOfQYa3ln/wqAm0rav+DuNxT/NhfoQoiBs2qwu/tjAPgvIYQQrwk28539djM7Zmb3mNlVWzYjIcRA2GiwfwnAGwDcAOAUgM+xO5rZYTM7amZHFxd5nnEhxGDZULC7+xl373k/3ceXAdwY3PeIu8+4+8z4ON/cEEIMlg0Fu5ntX/Hn+wE8szXTEUIMirVIb18H8A4Au83sBIBPA3iHmd2Avhb1IoCPr+VkZkZzmlWIvAZwx1O+gVJNQOx4irqc2ZAC6afd5nJMN5BjKkFJo0iWW1wod6m9VDlPx9SqNdrXDkpD7T2wb93H3DPN89adOnWa9kXXR5WUFAN4laRIYY3y043Xeb6+xSZ3CC4tBaWteuR8gRzNjhZdG6sGu7t/pKT57tXGCSGuLPQLOiESQcEuRCIo2IVIBAW7EImgYBciEYaecJLD5Y6sSqbZi5JDclkukjQqoZuo/HzdoPxQYGxDFshJUdmlPJCGjGhKi4sLdMz4GP+xU7vDpbcL57mcZ7vK2yenpuiYybk52rfU4uWwPHjPMraOkawVuegC9932qW20b3mJr2NOEm1SqRerXadsjBAiCRTsQiSCgl2IRFCwC5EICnYhEkHBLkQiDF16o26uwK0DIxJE5FwKOwPJLliRGqkt1wssVJUgYWNElDAzkl16ZB0jOWlpibu1mk3u8oqoZWQhA8nr0NVX077nn3+Bnyx4PqmDLUpWGiR6bAW17ybGJ2jf+ARfx7n5+fJ5eFD7LpBfGXpnFyIRFOxCJIKCXYhEULALkQgKdiESYfhGGLLRGW3Gsx38PBgUlVbyYCc2ywNTRbX8fFnO52HBucKd+kicCAw0aBNTTiBOdHOeC68T5NBrjNVp30vnystQ7dq9m45pjo3Rvl27dtK+ixd4DRNmhOkF5iUEO909kg8RACpBLrzJwCRzeaE8xTrLvQjEag1D7+xCJIKCXYhEULALkQgKdiESQcEuRCIo2IVIhLWUf7oawFcB7EVfwDni7l80s50AvgHgGvRLQN3i7hdXORbqtXK5JvphP1ONIhmEnQcAukF+uiwou8TmMdbkOdy63Q7ti0r1RESSDEt6V4mMH4HhotPlstxSkFetS8ZVmEEGQCcwkkzv4WWjWks8P91yp1w6zLIgx1/kMWGlmgDkPX5dNar8emSlylpB1eOKrd9gtZZ39i6AT7n7dQDeAuATZnYdgDsAPOru1wJ4tPhbCHGFsmqwu/spd/9JcXsewLMADgK4GcC9xd3uBfC+QU1SCLF51vWd3cyuAfAmAI8D2Ovup4qu0+h/zBdCXKGsOdjNbBLAAwA+6e6vSPDt/d+fln4pNLPDZnbUzI4uLPDc5UKIwbKmYDezGvqB/jV3f7BoPmNm+4v+/QDOlo119yPuPuPuMxMTfANGCDFYVg12628Z3w3gWXf//IquhwDcWty+FcC3t356QoitYi2ut7cC+CiAp83syaLtTgCfAXC/md0G4DiAW9ZyQt+A2pSR16QscI1FqlY1co1FbjNy0Eog41QCF11W4ctvLO8egDaRk/rj1m8r9B6X16JcflE+tm2kzFOjwSWoeqNG+6LLZt+B/bTv18ePl7ZH8ms1eF7CmUQOR1LiCQAmxydL25eD9Q1MjJRVg93dfwj+CN+1gXMKIUaAfkEnRCIo2IVIBAW7EImgYBciERTsQiTCCMo/kY1953JYpVb+mlQl5ZgAoBIknOwGUhMtTwUuG9aqfB5ZVMYpSFCYRzJO4ERjjy1yykUJOCPywKm4QBxbnQ6f++zsLO07eOgQ7Rsf565DJs922tyNGJXKagcuxk6Xy3n1KpcV62PlfZE02yHya/Rc6p1diERQsAuRCAp2IRJBwS5EIijYhUgEBbsQiTBU6c3MkBGZKko4yZxckeQVqUm1QAaJ5tFolNciywIJLXLm5VEtr+gBBNbBCunKAwnQA4dd6PIK3io6nXKJqsImCCBf5utx/H9fpH1TU+WuMQCo18uf61aLJ6mMnH5RNspuUBevFshotax8jlHtu5zIg1ESU72zC5EICnYhEkHBLkQiKNiFSAQFuxCJMNzdeBhqpGxNL9hFpCWjgl1TC3Z9G7UG7asFZaPySvn52ot8Z/f8xfO0L8oLlyHIr5fxccYMRcYNKMiDHfLAdMNKTQHckNHt8pJGExN8Vz0yL83NzdE+VppriuTIA4B2K8jxFzzmXmCg6eZ8/uz6jrIxM0OOduOFEAp2IVJBwS5EIijYhUgEBbsQiaBgFyIRVpXezOxqAF9FvySzAzji7l80s7sAfAzAS8Vd73T3h+NjAVmNSEOB/BOVV2KEZaai0lCBqYX11YLXzMsLl2kfyyMGxOaUMM8YlYb443KPyj9xohJVGcnl1wlyuLWWeLmjiUkuQ3UCebDdWS4/XpMfj16jiK+ryEQVOrOIXFZvcIk4IyXMIultLTp7F8Cn3P0nZjYF4Akze6To+4K7/+MajiGEGDFrqfV2CsCp4va8mT0L4OCgJyaE2FrW9fnYzK4B8CYAjxdNt5vZMTO7x8yu2uK5CSG2kDUHu5lNAngAwCfdfQ7AlwC8AcAN6L/zf46MO2xmR83s6OWFhS2YshBiI6wp2M2shn6gf83dHwQAdz/j7j3v70p8GcCNZWPd/Yi7z7j7zGTwW18hxGBZNditv713N4Bn3f3zK9r3r7jb+wE8s/XTE0JsFWvZjX8rgI8CeNrMniza7gTwETO7AX115kUAH1/1SGao1srzbXlQFsiI9Ga82k4oQSBQSHLwg45PljullgOX1LZt22nf7OxFPo8N5qBj8k/kAqwQGadP4HoLoPJg8LCYTAYAjS7Px1Yn1xQAtNvlx+w1+PPcHG/Svl6Pj4vKRkWSHXvc9QZ3YI5PlLv5orJna9mN/yHKlelQUxdCXFnoF3RCJIKCXYhEULALkQgKdiESQcEuRCIMPeGkEctZpcpfd5iDqhckUaxmQeLInMsnFrz+ZVn5cjWbfEynw5MoXr7MEyV22pEUyaWyLim7xMpuAUBuXIuMHHYW6ElU+oxcYz1+rvl5vlbTu3bTPiYr5l1+DVTGgmsgcsTRnlje7BHXXqS+suSckfSmd3YhEkHBLkQiKNiFSAQFuxCJoGAXIhEU7EIkwlClt063gzOnT5f27T2wj46rEBnHA5khol7nifx6OZehFi+X1ynLgoSY9QZ3ZG3fwZP7zF6YpX1MXgOAKquLFzyuSJZjciMAIEg4yeTNMCFila9VL7AqtpZ5rT3mYMsDma8T1JWbGCt3mwGxs60buDpZItNekJwzIzUTI/TOLkQiKNiFSAQFuxCJoGAXIhEU7EIkgoJdiEQYqvTWarXwi1/9srRv1zR3LtWb5XJHJJMRo1x/XFCTq258SVj9tcjtlAXy4ESTJzZsT/AklnNzXJbjSQ8D9xrtASyLEncGj5w97ihhYzDHSqBrLS3zRJUsaeNGXGgAkAfzrwUJIiP3ID1X5DisrP94emcXIhEU7EIkgoJdiERQsAuRCAp2IRJh1d14MxsD8BiARnH/b7r7p83sdQDuA7ALwBMAPurufAsZQLfXxYWL50v7Ll26RMdNkIKQGTEQAECwUQ8Pctf1cv76l5Pd5yB9Hh0DxAaUZpOXO+p1eV67peWl8vYlbhYJd4qjkkbhPn75OJaDsD8iOtf6TTcAsLhY/ri3bysv5dU/ID9XNzhXJdipr9e4+YrlAPQuv4h7y6y8VqBo0J7fsQzgne7+RvTLM99kZm8B8FkAX3D3PwRwEcBtaziWEGJErBrs3udy8Wet+OcA3gngm0X7vQDeN5AZCiG2hLXWZ8+KCq5nATwC4HkAs+7+8ufhEwAODmaKQoitYE3B7u49d78BwCEANwL4o7WewMwOm9lRMzvaaXMzvhBisKxrN97dZwF8H8CfAthh9v+/LT0E4CQZc8TdZ9x9plbnmUiEEINl1WA3s2kz21HcbgJ4N4Bn0Q/6DxR3uxXAtwc1SSHE5lmLEWY/gHvNLEP/xeF+d/+umf0MwH1m9vcA/gfA3aseyQHmQTlNctMBwP59+0vbozxcnTwwMwS6nAcGAyOyRh7IQllguAhLKwW52sYCWW6ZmEIqwbl6oaGFdwUp6MDsNVaJErVF0ltA0Nkl+eS6QfmnaljiKThZmF8vkonLxy1H60Gu02idVg12dz8G4E0l7S+g//1dCPEaQL+gEyIRFOxCJIKCXYhEULALkQgKdiESwTaSG2vDJzN7CcDx4s/dAM4N7eQczeOVaB6v5LU2jz9w9+myjqEG+ytObHbU3WdGcnLNQ/NIcB76GC9EIijYhUiEUQb7kRGeeyWaxyvRPF7J7808RvadXQgxXPQxXohEGEmwm9lNZvYLM3vOzO4YxRyKebxoZk+b2ZNmdnSI573HzM6a2TMr2naa2SNm9qvi/6tGNI+7zOxksSZPmtl7hzCPq83s+2b2MzP7qZn9VdE+1DUJ5jHUNTGzMTP7kZk9Vczj74r215nZ40XcfMPMeL2pMtx9qP8AZOintXo9gDqApwBcN+x5FHN5EcDuEZz37QDeDOCZFW3/AOCO4vYdAD47onncBeCvh7we+wG8ubg9BeCXAK4b9poE8xjqmqDvD54sbtcAPA7gLQDuB/Dhov2fAfzleo47inf2GwE85+4veD/19H0Abh7BPEaGuz8G4MKrmm9GP3EnMKQEnmQeQ8fdT7n7T4rb8+gnRzmIIa9JMI+h4n22PMnrKIL9IIDfrPh7lMkqHcD3zOwJMzs8ojm8zF53P1XcPg1g7wjncruZHSs+5g/868RKzOwa9PMnPI4Rrsmr5gEMeU0GkeQ19Q26t7n7mwH8OYBPmNnbRz0hoP/KjlWSswyQLwF4A/o1Ak4B+NywTmxmkwAeAPBJd59b2TfMNSmZx9DXxDeR5JUximA/CeDqFX/TZJWDxt1PFv+fBfAtjDbzzhkz2w8Axf9nRzEJdz9TXGg5gC9jSGtiZjX0A+xr7v5g0Tz0NSmbx6jWpDj3upO8MkYR7D8GcG2xs1gH8GEADw17EmY2YWZTL98G8B4Az8SjBspD6CfuBEaYwPPl4Cp4P4awJtZPuHc3gGfd/fMruoa6Jmwew16TgSV5HdYO46t2G9+L/k7n8wD+ZkRzeD36SsBTAH46zHkA+Dr6Hwc76H/3ug39mnmPAvgVgP8CsHNE8/hXAE8DOIZ+sO0fwjzehv5H9GMAniz+vXfYaxLMY6hrAuBP0E/iegz9F5a/XXHN/gjAcwD+HUBjPcfVL+iESITUN+iESAYFuxCJoGAXIhEU7EIkgoJdiERQsAuRCAp2IRJBwS5EIvwfolyK1S1AUd4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am49dfzPkpqm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}