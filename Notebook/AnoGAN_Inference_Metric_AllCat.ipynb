{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "AnoGAN_Inference_Metric_AllCat.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plodha/CMPE-297-DeepLearning/blob/main/Notebook/AnoGAN_Inference_Metric_AllCat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCzmSViMljTt"
      },
      "source": [
        "# Mount Drive and Set Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GImoCE9Uk0CF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a3fc26-7826-4581-86f6-1d80f45a68ff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNhf6fvzk3Lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be9dc12f-5712-4fa5-a14c-d84019329f1a"
      },
      "source": [
        "!pip install torch==1.7.0 torchvision==0.5.0 tqdm opencv-python Pillow==8.0.1 tensorboardX==1.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Collecting torchvision==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Collecting Pillow==8.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/19/d4c25111d36163698396f93c363114cf1cddbacb24744f6612f25b6aa3d0/Pillow-8.0.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 60.2MB/s \n",
            "\u001b[?25hCollecting tensorboardX==1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/d2/e08fe62f3554fbba081e80f6b23128df53b2f74ed4dcde73ec4a84dc53fb/tensorboardX-1.4-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.4) (50.3.2)\n",
            "\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.7.0+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow, torchvision, tensorboardX\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Successfully installed Pillow-8.0.1 tensorboardX-1.4 torchvision-0.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9XQVZ8yk5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40552ae6-c476-4b72-ba31-13a05afc229d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Dec  5 21:43:41 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvWbMV8Lk7mB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb28b471-2b57-4922-d789-cc0fe847fbdf"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktBKO9rWkpqi"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "import random\n",
        "import pandas as pd\n",
        "#import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "#from dataloader.dataloader import load_data\n",
        "#from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "#from networks import Generator, Discriminator\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "ngpu = 1\n",
        "os.makedirs(\"images\", exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32g4Nghbkpqj"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "class MURA_dataset(Dataset):\n",
        "    '''\n",
        "    Dataset class for MURA dataset\n",
        "    Args:\n",
        "        - df: Dataframe with the first columns contains the path to the images\n",
        "        - root_dir: string contains path of  root directory\n",
        "        - transforms: Pytorch transform operations\n",
        "    '''\n",
        "\n",
        "    def __init__(self, df, root_dir, transforms=None):\n",
        "        #print(\"I am calling Mura dataset\")\n",
        "        self.df = df\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.df.iloc[idx, 0])\n",
        "        #print('img_name ',img_name)\n",
        "        img = cv2.imread(img_name)\n",
        "        #print('img shape ',img.shape)\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        if 'negative' in img_name: label = 0\n",
        "        else: label = 1\n",
        "\n",
        "        return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hqNZtKDkpqj"
      },
      "source": [
        "def transform(rotation, hflip, resize, totensor, normalize, centercrop, to_pil, gray):\n",
        "    options = []\n",
        "    if to_pil:\n",
        "        options.append(torchvision.transforms.ToPILImage())\n",
        "    if gray:\n",
        "        options.append(torchvision.transforms.Grayscale())\n",
        "    if rotation:\n",
        "        options.append(torchvision.transforms.RandomRotation(20))\n",
        "    if hflip:\n",
        "        options.append(torchvision.transforms.RandomHorizontalFlip())\n",
        "    if centercrop:\n",
        "        options.append(torchvision.transforms.CenterCrop(256))\n",
        "    if resize:\n",
        "        options.append(torchvision.transforms.Resize((32,32)))\n",
        "    if totensor:\n",
        "        options.append(torchvision.transforms.ToTensor())\n",
        "    # if True:\n",
        "    #     options.append(transforms.Lambda(lambda x: (x - x.min())/(x.max()-x.min())))\n",
        "    if normalize:\n",
        "        options.append(torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
        "    transform = torchvision.transforms.Compose(options)\n",
        "    return transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar_SQ2hhkpqj"
      },
      "source": [
        "def customDf(path, studyClass=None, studyType=None):\n",
        "    '''\n",
        "    Function to get custom csv based on class of study and type of study\n",
        "    Args:\n",
        "        - path(string): path to original csv\n",
        "        - studyClass(list): class of study, list must contains one of the following:\n",
        "            \"XR_ELBOW\",\n",
        "            \"XR_FINGER\",\n",
        "            \"XR_FOREARM\",\n",
        "            \"XR_HAND\",\n",
        "            \"XR_HUMERUS\",\n",
        "            \"XR_SHOULDER\",\n",
        "            \"XR_WRIST\"\n",
        "            if None, take all\n",
        "        - studyResult(list): Result of study, list must contains one of the following:\n",
        "            \"positive\", \"negative\"\n",
        "            if None, take all\n",
        "    '''\n",
        "    df = pd.read_csv(path, header=None)\n",
        "\n",
        "    if studyClass:\n",
        "        cond = df[0].str.contains(studyClass)\n",
        "        df = df[cond]\n",
        "    if studyType:\n",
        "        cond = df[0].str.contains(studyType)\n",
        "        df = df[cond]\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-A8jeDGkpqj"
      },
      "source": [
        "#import pandas as pd\n",
        "#import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyMShdYgkpqj"
      },
      "source": [
        "#valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MZ4oVUFkpqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0215de-0a92-4715-e78c-d067835c16fc"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGrV7RkUkpql"
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdSxrnohkpql"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umqs0EXGkpql"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, dim, zdim, nc):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.dim = dim\n",
        "        preprocess = nn.Sequential(\n",
        "            nn.Linear(zdim, 4 * 4 * 4 * dim),\n",
        "            nn.BatchNorm1d(4 * 4 * 4 * dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        block1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(4 * dim, 2 * dim, 2, stride=2),\n",
        "            nn.BatchNorm2d(2 * dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        block2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2 * dim, dim, 2, stride=2),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        deconv_out = nn.ConvTranspose2d(dim, nc, 2, stride=2)\n",
        "\n",
        "        self.preprocess = preprocess\n",
        "        self.block1 = block1\n",
        "        self.block2 = block2\n",
        "        self.deconv_out = deconv_out\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.preprocess(input)\n",
        "        output = output.view(-1, 4 * self.dim, 4, 4)\n",
        "        output = self.block1(output)\n",
        "        output = self.block2(output)\n",
        "        output = self.deconv_out(output)\n",
        "        output = self.tanh(output)\n",
        "        return output.view(-1, self.nc, 32, 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAqyGQqQkpql"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qfMbK1skpql"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, dim, zdim, nc, out_feat=False):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.dim = dim\n",
        "        main = nn.Sequential(\n",
        "            nn.Conv2d(nc, dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(dim, 2 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(2 * dim, 4 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.out_feat=out_feat\n",
        "        self.main = main\n",
        "        self.linear = nn.Linear(4*4*4*dim, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        output = output.view(-1, 4*4*4*self.dim)\n",
        "        if self.out_feat:\n",
        "            return output\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "     def __init__(self,dim, zdim, nc):\n",
        "         super(Encoder, self).__init__()\n",
        "         self.dim = dim\n",
        "         main = nn.Sequential(\n",
        "            nn.Conv2d(nc, dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(dim, 2 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(2 * dim, 4 * dim, 3, 2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            )\n",
        "         self.main = main\n",
        "         self.linear = nn.Linear(4*4*4*dim, zdim)\n",
        "\n",
        "     def forward(self, input):\n",
        "         output = self.main(input)\n",
        "         output = output.view(-1, 4*4*4*self.dim)\n",
        "         output = self.linear(output)\n",
        "         return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5voHg2ZOkpql"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbMKsARzhpw5"
      },
      "source": [
        "## Category \n",
        "arr = [\"XR_ELBOW\",\"XR_FINGER\",\"XR_FOREARM\",\"XR_HAND\",\"XR_HUMERUS\",\"XR_SHOULDER\",\"XR_WRIST\"]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0k4u1sjkpql"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP5Fb1IK37Ww"
      },
      "source": [
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
        "\n",
        "    Parameters:\n",
        "        input_image (tensor) --  the input image tensor array\n",
        "        imtype (type)        --  the desired type of the converted numpy array\n",
        "    \"\"\"\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor[0].cpu().float().numpy()  # convert it into a numpy array\n",
        "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
        "    else:  # if it is a numpy array, do nothing\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHpX_cFE7dSC"
      },
      "source": [
        "## 1.XR_ELBOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0ztfl0l7dSC"
      },
      "source": [
        "# change the path here to shared drive\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)    #scs\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)\n",
        "\n",
        "\n",
        "mura_valid_df = customDf('/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/MURA-v1.1/valid_image_paths.csv', 'XR_ELBOW', None)\n",
        "valid_dataset = MURA_dataset(mura_valid_df, '/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/', transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN-cJLnu7dSC"
      },
      "source": [
        "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVabEIqi7dSC"
      },
      "source": [
        "### Metric Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QUSRXTC7dSC",
        "outputId": "41f20452-7f55-4704-ad39-605b284d74a3"
      },
      "source": [
        "\n",
        "\n",
        "n_epochs = 5001\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 128\n",
        "img_size = 64\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "abnormal_class = 0\n",
        "#device = 'cuda' \n",
        "out = '/content/drive/Shared drives/MeanSquare-Drive/RL-Project/AnoGAN/models/anoGAN-ckpts-XR_ELBOW/' #scs- change here for all cat\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "max_auc = 0\n",
        "\n",
        "\n",
        "generator = Generator(dim = 64, zdim=latent_dim, nc=channels)\n",
        "discriminator = Discriminator(dim = 64, zdim=latent_dim, nc=channels,out_feat=True)\n",
        "encoder = Encoder(dim = 64, zdim=latent_dim, nc=channels)\n",
        "\n",
        "generator.load_state_dict(torch.load(out+'G_epoch5000.pt'))\n",
        "discriminator.load_state_dict(torch.load(out+'D_epoch5000.pt'))\n",
        "generator.to(device)\n",
        "encoder.to(device)\n",
        "discriminator.to(device)\n",
        "with torch.no_grad():\n",
        "    labels = torch.zeros(size=(len(valid_dataloader.dataset),),\n",
        "                                        dtype=torch.long, device=device)\n",
        "\n",
        "    scores = torch.empty(\n",
        "                size=(len(valid_dataloader.dataset),),\n",
        "                dtype=torch.float32,\n",
        "                device=device)\n",
        "    for i, (imgs, lbls) in enumerate(valid_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "\n",
        "            labels[i*batch_size:(i+1)*batch_size].copy_(lbls)\n",
        "            emb_query = encoder(imgs)\n",
        "            fake_imgs = generator(emb_query)\n",
        "            emb_fake = encoder(fake_imgs)\n",
        "\n",
        "            image_feats  = discriminator(imgs)\n",
        "            recon_feats = discriminator(fake_imgs)\n",
        "                \n",
        "            diff = imgs-fake_imgs\n",
        "            \n",
        "            image1_tensor= diff[0]\n",
        "            im = tensor2im(imgs)\n",
        "            \n",
        "            im2 = tensor2im(fake_imgs)\n",
        "            print(lbls)\n",
        "            \n",
        "            im3 = tensor2im(diff)\n",
        "            plt.figure(1)\n",
        "            plt.subplot(311)\n",
        "            plt.title('Real image')\n",
        "            plt.imshow(im)\n",
        "\n",
        "            plt.subplot(312)\n",
        "            plt.title('Fake img')\n",
        "            plt.imshow(im2)\n",
        "            plt.show()\n",
        "            \n",
        "            img = cv2.GaussianBlur(im3,(5,5),0)\n",
        "            img_gray = rgb2gray(img)\n",
        "            plt.imshow(img_gray)\n",
        "            thresh = threshold_otsu(img_gray)\n",
        "            binary = img_gray > thresh\n",
        "            \n",
        "            plt.imshow(binary)\n",
        "            im_rgb = np.array(Image.fromarray(binary).convert('RGB'))\n",
        "            mask = binary.copy()\n",
        "            mask[mask > 0.5] = 1\n",
        "            mask[mask <= 0.5] = 0\n",
        "            \n",
        "            mask3 = np.stack((mask,mask,mask), axis=2)\n",
        "\n",
        "            all_labels = measure.label(mask)\n",
        "            all_labels[all_labels >= 1] = 255\n",
        "            all_labels[all_labels < 1] = 0\n",
        "            all_labels3 = np.stack((all_labels,all_labels,all_labels), axis=2)\n",
        "            \n",
        "           \n",
        "            \n",
        "            \n",
        "#             kernel = np.ones((6, 6), np.uint8) \n",
        "  \n",
        "#             # Using cv2.erode() method  \n",
        "#             image = cv2.erode(Image.fromarray(mask3), kernel, cv2.BORDER_REFLECT) \n",
        "        \n",
        "            black_pixels_mask = np.all(mask3 == 1, axis=2)\n",
        "            non_black_pixels_mask = np.any(mask3 > [0, 0, 0], axis=-1)\n",
        "          \n",
        "            all_labels3[non_black_pixels_mask] = [255,0,0]\n",
        "            \n",
        "            plt.subplot(313)\n",
        "            plt.title('Difference')\n",
        "            plt.imshow(im3)\n",
        "            plt.show()\n",
        "            \n",
        "            plt.subplot(321)\n",
        "            plt.title('colored mask')\n",
        "            plt.imshow(all_labels3)\n",
        "            plt.show()\n",
        "            \n",
        "            \n",
        "            \n",
        "            gray = cv2.cvtColor(im3, cv2.COLOR_BGR2GRAY) \n",
        "  \n",
        "            # Find Canny edges \n",
        "            edged = cv2.Canny(gray, 30, 200) \n",
        "           \n",
        "\n",
        "            # Finding Contours \n",
        "            # Use a copy of the image e.g. edged.copy() \n",
        "            # since findContours alters the image \n",
        "            contours, hierarchy = cv2.findContours(edged,  \n",
        "                cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
        "\n",
        "            plt.subplot(322)\n",
        "            plt.imshow(edged)\n",
        "            plt.title('Edged')\n",
        "            plt.show()\n",
        "            \n",
        "\n",
        "            print(\"Number of Contours found = \" + str(len(contours))) \n",
        "\n",
        "            # Draw all contours \n",
        "            # -1 signifies drawing all contours \n",
        "            print('im3: ',im3.shape)\n",
        "            cv2.drawContours(gray, contours, -1, (0, 255, 0), 3) \n",
        "\n",
        "            plt.subplot(323)\n",
        "            plt.title('contour')\n",
        "            plt.imshow(gray)\n",
        "            plt.show()\n",
        "           \n",
        "            break    \n",
        "            \n",
        "            image_distance = torch.mean(torch.pow(imgs-fake_imgs, 2), dim=[1,2,3])\n",
        "            feat_distance = torch.mean(torch.pow(image_feats-recon_feats, 2), dim=1)\n",
        "            print(emb_query.shape, emb_fake.shape)\n",
        "            z_distance = mse_loss(emb_query, emb_fake)#mse_loss(emb_query, emb_fake)\n",
        "            #print z_distance\n",
        "            print('z_distance=',z_distance)\n",
        "            #print('hiiiiiiiii')\n",
        "            scores[i*batch_size:(i+1)*batch_size].copy_(feat_distance)\n",
        "            break\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    # scores = torch.mean(scores,)\n",
        "    scores = scores.cpu().squeeze()\n",
        "    print(scores.shape)\n",
        "\n",
        "    #print('\\n####################')\n",
        "    print('\\n######## Category: XR_ELBOW #######')\n",
        "    \n",
        "    \n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('roc_auc=', roc_auc)\n",
        "    max_auc = max(roc_auc, max_auc)\n",
        "    print('max_auc=', max_auc)\n",
        "    \n",
        "    print(len(valid_dataloader.dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "torch.Size([64, 128]) torch.Size([64, 128])\n",
            "z_distance= tensor(0.0003, device='cuda:0')\n",
            "torch.Size([465])\n",
            "\n",
            "######## Category: XR_ELBOW #######\n",
            "roc_auc= 0.8844511230623221\n",
            "max_auc= 0.8844511230623221\n",
            "465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDjrGfpEIzMW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hin4opaL70vL"
      },
      "source": [
        "## 2.XR_FINGER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZxTKh6l70vL"
      },
      "source": [
        "# change the path here to shared drive\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)    #scs\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)\n",
        "\n",
        "\n",
        "mura_valid_df = customDf('/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/MURA-v1.1/valid_image_paths.csv', 'XR_FINGER', None)\n",
        "valid_dataset = MURA_dataset(mura_valid_df, '/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/', transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv-ES13q70vL"
      },
      "source": [
        "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlJBefVy70vL"
      },
      "source": [
        "### Metric Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "y4j_Mg7d70vL",
        "outputId": "1108b9b0-e757-4264-af9d-55f4222116f6"
      },
      "source": [
        "\n",
        "\n",
        "n_epochs = 5001\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 128\n",
        "img_size = 64\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "abnormal_class = 0\n",
        "#device = 'cuda' \n",
        "out = '/content/drive/Shared drives/MeanSquare-Drive/RL-Project/AnoGAN/models/anoGAN-ckpts-XR_FINGER/' #scs- change here for all cat\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "max_auc = 0\n",
        "\n",
        "\n",
        "generator = Generator(dim = 64, zdim=latent_dim, nc=channels)\n",
        "discriminator = Discriminator(dim = 64, zdim=latent_dim, nc=channels,out_feat=True)\n",
        "encoder = Encoder(dim = 64, zdim=latent_dim, nc=channels)\n",
        "\n",
        "generator.load_state_dict(torch.load(out+'G_epoch5000.pt'))\n",
        "discriminator.load_state_dict(torch.load(out+'D_epoch5000.pt'))\n",
        "generator.to(device)\n",
        "encoder.to(device)\n",
        "discriminator.to(device)\n",
        "with torch.no_grad():\n",
        "    labels = torch.zeros(size=(len(valid_dataloader.dataset),),\n",
        "                                        dtype=torch.long, device=device)\n",
        "\n",
        "    scores = torch.empty(\n",
        "                size=(len(valid_dataloader.dataset),),\n",
        "                dtype=torch.float32,\n",
        "                device=device)\n",
        "    for i, (imgs, lbls) in enumerate(valid_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "\n",
        "            labels[i*batch_size:(i+1)*batch_size].copy_(lbls)\n",
        "            emb_query = encoder(imgs)\n",
        "            fake_imgs = generator(emb_query)\n",
        "            emb_fake = encoder(fake_imgs)\n",
        "\n",
        "            image_feats  = discriminator(imgs)\n",
        "            recon_feats = discriminator(fake_imgs)\n",
        "                \n",
        "            diff = imgs-fake_imgs\n",
        "            \n",
        "            image1_tensor= diff[0]\n",
        "           \n",
        "            im = tensor2im(imgs)\n",
        "            plt.imshow(im)\n",
        "            \n",
        "            im2 = tensor2im(fake_imgs)\n",
        "            plt.imshow(im2)\n",
        "            \n",
        "            im3 = tensor2im(diff)\n",
        "            plt.imshow(im3)\n",
        "            print(im.shape)\n",
        "            print(im3.shape)\n",
        "            #break   \n",
        "            \n",
        "            image_distance = torch.mean(torch.pow(imgs-fake_imgs, 2), dim=[1,2,3])\n",
        "            feat_distance = torch.mean(torch.pow(image_feats-recon_feats, 2), dim=1)\n",
        "            print(emb_query.shape, emb_fake.shape)\n",
        "            z_distance = mse_loss(emb_query, emb_fake)#mse_loss(emb_query, emb_fake)\n",
        "            #print z_distance\n",
        "            print('z_distance=',z_distance)\n",
        "            #print('hiiiiiiiii')\n",
        "            scores[i*batch_size:(i+1)*batch_size].copy_(feat_distance)\n",
        "            break\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    # scores = torch.mean(scores,)\n",
        "    scores = scores.cpu().squeeze()\n",
        "    print(scores.shape)\n",
        "\n",
        "    #print('\\n####################')\n",
        "    print('\\n######## Category: XR_FINGER #######')\n",
        "    \n",
        "    \n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('roc_auc=', roc_auc)\n",
        "    max_auc = max(roc_auc, max_auc)\n",
        "    print('max_auc=', max_auc)\n",
        "    \n",
        "    print(len(valid_dataloader.dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "torch.Size([64, 128]) torch.Size([64, 128])\n",
            "z_distance= tensor(0.0002, device='cuda:0')\n",
            "torch.Size([461])\n",
            "\n",
            "######## Category: XR_FINGER #######\n",
            "roc_auc= 0.9261363636363636\n",
            "max_auc= 0.9261363636363636\n",
            "461\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD8cusw50jT2"
      },
      "source": [
        "## 3.XR_FOREARM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-pYUr580jT2"
      },
      "source": [
        "# change the path here to shared drive\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)    #scs\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)\n",
        "\n",
        "\n",
        "mura_valid_df = customDf('/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/MURA-v1.1/valid_image_paths.csv', 'XR_FOREARM', None)\n",
        "valid_dataset = MURA_dataset(mura_valid_df, '/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/', transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYDPphdl0jT2"
      },
      "source": [
        "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP_GgqJz0jT2"
      },
      "source": [
        "### Metric Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "deBLkQRM0jT2",
        "outputId": "ec5ef450-c560-4ac2-9451-16c2d91a2b2a"
      },
      "source": [
        "\n",
        "\n",
        "n_epochs = 5001\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 128\n",
        "img_size = 64\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "abnormal_class = 0\n",
        "#device = 'cuda' \n",
        "out = '/content/drive/Shared drives/MeanSquare-Drive/RL-Project/AnoGAN/models/anoGAN-ckpts-XR_FOREARM/' #scs- change here for all cat\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "max_auc = 0\n",
        "\n",
        "\n",
        "generator = Generator(dim = 64, zdim=latent_dim, nc=channels)\n",
        "discriminator = Discriminator(dim = 64, zdim=latent_dim, nc=channels,out_feat=True)\n",
        "encoder = Encoder(dim = 64, zdim=latent_dim, nc=channels)\n",
        "\n",
        "generator.load_state_dict(torch.load(out+'G_epoch5000.pt'))\n",
        "discriminator.load_state_dict(torch.load(out+'D_epoch5000.pt'))\n",
        "generator.to(device)\n",
        "encoder.to(device)\n",
        "discriminator.to(device)\n",
        "with torch.no_grad():\n",
        "    labels = torch.zeros(size=(len(valid_dataloader.dataset),),\n",
        "                                        dtype=torch.long, device=device)\n",
        "\n",
        "    scores = torch.empty(\n",
        "                size=(len(valid_dataloader.dataset),),\n",
        "                dtype=torch.float32,\n",
        "                device=device)\n",
        "    for i, (imgs, lbls) in enumerate(valid_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "\n",
        "            labels[i*batch_size:(i+1)*batch_size].copy_(lbls)\n",
        "            emb_query = encoder(imgs)\n",
        "            fake_imgs = generator(emb_query)\n",
        "            emb_fake = encoder(fake_imgs)\n",
        "\n",
        "            image_feats  = discriminator(imgs)\n",
        "            recon_feats = discriminator(fake_imgs)\n",
        "                \n",
        "            diff = imgs-fake_imgs\n",
        "            \n",
        "            image1_tensor= diff[0]\n",
        "           \n",
        "            im = tensor2im(imgs)\n",
        "            plt.imshow(im)\n",
        "            \n",
        "            im2 = tensor2im(fake_imgs)\n",
        "            plt.imshow(im2)\n",
        "            \n",
        "            im3 = tensor2im(diff)\n",
        "            plt.imshow(im3)\n",
        "            print(im.shape)\n",
        "            print(im3.shape)\n",
        "            #break   \n",
        "            \n",
        "            image_distance = torch.mean(torch.pow(imgs-fake_imgs, 2), dim=[1,2,3])\n",
        "            feat_distance = torch.mean(torch.pow(image_feats-recon_feats, 2), dim=1)\n",
        "            print(emb_query.shape, emb_fake.shape)\n",
        "            z_distance = mse_loss(emb_query, emb_fake)#mse_loss(emb_query, emb_fake)\n",
        "            #print z_distance\n",
        "            print('z_distance=',z_distance)\n",
        "            #print('hiiiiiiiii')\n",
        "            scores[i*batch_size:(i+1)*batch_size].copy_(feat_distance)\n",
        "            break\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    # scores = torch.mean(scores,)\n",
        "    scores = scores.cpu().squeeze()\n",
        "    print(scores.shape)\n",
        "\n",
        "    #print('\\n####################')\n",
        "    print('\\n######## Category: XR_FOREARM #######')\n",
        "    \n",
        "    \n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('roc_auc=', roc_auc)\n",
        "    max_auc = max(roc_auc, max_auc)\n",
        "    print('max_auc=', max_auc)\n",
        "    \n",
        "    print(len(valid_dataloader.dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "torch.Size([64, 128]) torch.Size([64, 128])\n",
            "z_distance= tensor(0.0002, device='cuda:0')\n",
            "torch.Size([301])\n",
            "\n",
            "######## Category: XR_FOREARM #######\n",
            "roc_auc= 0.9324966078697422\n",
            "max_auc= 0.9324966078697422\n",
            "301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fa7Ve8G0jT2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXMaCmVv6j6i"
      },
      "source": [
        "## 4.XR_HAND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rWr4k5J6j6i"
      },
      "source": [
        "# change the path here to shared drive\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)    #scs\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)\n",
        "\n",
        "\n",
        "mura_valid_df = customDf('/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/MURA-v1.1/valid_image_paths.csv', 'XR_HAND', None)\n",
        "valid_dataset = MURA_dataset(mura_valid_df, '/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/', transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-003ggDZ6j6j"
      },
      "source": [
        "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNiUiIIv6j6j"
      },
      "source": [
        "### Metric Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnXKZGcM6j6j"
      },
      "source": [
        "from torch.nn.functional import mse_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ysWxdZ16j6j",
        "outputId": "876d4c70-88c9-4b55-870b-1181a02dcd96"
      },
      "source": [
        "\n",
        "\n",
        "n_epochs = 5001\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 128\n",
        "img_size = 64\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "abnormal_class = 0\n",
        "#device = 'cuda' \n",
        "out = '/content/drive/Shared drives/MeanSquare-Drive/RL-Project/AnoGAN/models/anoGAN-ckpts-XR_HAND/' #scs- change here for all cat\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "max_auc = 0\n",
        "\n",
        "\n",
        "generator = Generator(dim = 64, zdim=latent_dim, nc=channels)\n",
        "discriminator = Discriminator(dim = 64, zdim=latent_dim, nc=channels,out_feat=True)\n",
        "encoder = Encoder(dim = 64, zdim=latent_dim, nc=channels)\n",
        "\n",
        "generator.load_state_dict(torch.load(out+'G_epoch5000.pt'))\n",
        "discriminator.load_state_dict(torch.load(out+'D_epoch5000.pt'))\n",
        "generator.to(device)\n",
        "encoder.to(device)\n",
        "discriminator.to(device)\n",
        "with torch.no_grad():\n",
        "    labels = torch.zeros(size=(len(valid_dataloader.dataset),),\n",
        "                                        dtype=torch.long, device=device)\n",
        "\n",
        "    scores = torch.empty(\n",
        "                size=(len(valid_dataloader.dataset),),\n",
        "                dtype=torch.float32,\n",
        "                device=device)\n",
        "    for i, (imgs, lbls) in enumerate(valid_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "\n",
        "            labels[i*batch_size:(i+1)*batch_size].copy_(lbls)\n",
        "            emb_query = encoder(imgs)\n",
        "            fake_imgs = generator(emb_query)\n",
        "            emb_fake = encoder(fake_imgs)\n",
        "\n",
        "            image_feats  = discriminator(imgs)\n",
        "            recon_feats = discriminator(fake_imgs)\n",
        "                \n",
        "            diff = imgs-fake_imgs\n",
        "            \n",
        "            image1_tensor= diff[0]\n",
        "           \n",
        "            im = tensor2im(imgs)\n",
        "            plt.imshow(im)\n",
        "            \n",
        "            im2 = tensor2im(fake_imgs)\n",
        "            plt.imshow(im2)\n",
        "            \n",
        "            im3 = tensor2im(diff)\n",
        "            plt.imshow(im3)\n",
        "            print(im.shape)\n",
        "            print(im3.shape)\n",
        "            #break   \n",
        "            \n",
        "            image_distance = torch.mean(torch.pow(imgs-fake_imgs, 2), dim=[1,2,3])\n",
        "            feat_distance = torch.mean(torch.pow(image_feats-recon_feats, 2), dim=1)\n",
        "            print(emb_query.shape, emb_fake.shape)\n",
        "            z_distance = mse_loss(emb_query, emb_fake)#mse_loss(emb_query, emb_fake)\n",
        "            #print z_distance\n",
        "            print('z_distance=',z_distance)\n",
        "            #print('hiiiiiiiii')\n",
        "            scores[i*batch_size:(i+1)*batch_size].copy_(feat_distance)\n",
        "            break\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    # scores = torch.mean(scores,)\n",
        "    scores = scores.cpu().squeeze()\n",
        "    print(scores.shape)\n",
        "\n",
        "    #print('\\n####################')\n",
        "    print('\\n######## Category: XR_HAND #######')\n",
        "    \n",
        "    \n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('roc_auc=', roc_auc)\n",
        "    max_auc = max(roc_auc, max_auc)\n",
        "    print('max_auc=', max_auc)\n",
        "    \n",
        "    print(len(valid_dataloader.dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "torch.Size([64, 128]) torch.Size([64, 128])\n",
            "z_distance= tensor(0.0002, device='cuda:0')\n",
            "torch.Size([460])\n",
            "\n",
            "######## Category: XR_HAND #######\n",
            "roc_auc= 0.7800925925925927\n",
            "max_auc= 0.7800925925925927\n",
            "460\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw_3_j5jhw1H"
      },
      "source": [
        "## 5.XR_HUMERUS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em71jeGqleMF"
      },
      "source": [
        "# change the path here to shared drive\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)    #scs\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)\n",
        "\n",
        "\n",
        "mura_valid_df = customDf('/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)\n",
        "valid_dataset = MURA_dataset(mura_valid_df, '/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/', transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW75BzS8lhRr"
      },
      "source": [
        "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX88VDNQsg0o"
      },
      "source": [
        "### Metric Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL0zW51a2ekm"
      },
      "source": [
        "from torch.nn.functional import mse_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPmVs1ckkpql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9ecdaf0-a45f-4ded-b3c2-34d232fb8bc1"
      },
      "source": [
        "\n",
        "\n",
        "n_epochs = 5001\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 128\n",
        "img_size = 64\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "abnormal_class = 0\n",
        "#device = 'cuda' \n",
        "out = '/content/drive/Shared drives/MeanSquare-Drive/RL-Project/AnoGAN/models/anoGAN-ckpts-XR_HUMERUS/' #scs- change here for all cat\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "max_auc = 0\n",
        "\n",
        "\n",
        "generator = Generator(dim = 64, zdim=latent_dim, nc=channels)\n",
        "discriminator = Discriminator(dim = 64, zdim=latent_dim, nc=channels,out_feat=True)\n",
        "encoder = Encoder(dim = 64, zdim=latent_dim, nc=channels)\n",
        "\n",
        "generator.load_state_dict(torch.load(out+'G_epoch5000.pt'))\n",
        "discriminator.load_state_dict(torch.load(out+'D_epoch5000.pt'))\n",
        "generator.to(device)\n",
        "encoder.to(device)\n",
        "discriminator.to(device)\n",
        "with torch.no_grad():\n",
        "    labels = torch.zeros(size=(len(valid_dataloader.dataset),),\n",
        "                                        dtype=torch.long, device=device)\n",
        "\n",
        "    scores = torch.empty(\n",
        "                size=(len(valid_dataloader.dataset),),\n",
        "                dtype=torch.float32,\n",
        "                device=device)\n",
        "    for i, (imgs, lbls) in enumerate(valid_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "\n",
        "            labels[i*batch_size:(i+1)*batch_size].copy_(lbls)\n",
        "            emb_query = encoder(imgs)\n",
        "            fake_imgs = generator(emb_query)\n",
        "            emb_fake = encoder(fake_imgs)\n",
        "\n",
        "            image_feats  = discriminator(imgs)\n",
        "            recon_feats = discriminator(fake_imgs)\n",
        "                \n",
        "            diff = imgs-fake_imgs\n",
        "            \n",
        "            image1_tensor= diff[0]\n",
        "           \n",
        "            im = tensor2im(imgs)\n",
        "            plt.imshow(im)\n",
        "            \n",
        "            im2 = tensor2im(fake_imgs)\n",
        "            plt.imshow(im2)\n",
        "            \n",
        "            im3 = tensor2im(diff)\n",
        "            plt.imshow(im3)\n",
        "            print(im.shape)\n",
        "            print(im3.shape)\n",
        "            #break   \n",
        "            \n",
        "            image_distance = torch.mean(torch.pow(imgs-fake_imgs, 2), dim=[1,2,3])\n",
        "            feat_distance = torch.mean(torch.pow(image_feats-recon_feats, 2), dim=1)\n",
        "            print(emb_query.shape, emb_fake.shape)\n",
        "            z_distance = mse_loss(emb_query, emb_fake)#mse_loss(emb_query, emb_fake)\n",
        "            #print z_distance\n",
        "            print('z_distance=',z_distance)\n",
        "            #print('hiiiiiiiii')\n",
        "            scores[i*batch_size:(i+1)*batch_size].copy_(feat_distance)\n",
        "            break\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    # scores = torch.mean(scores,)\n",
        "    scores = scores.cpu().squeeze()\n",
        "    print(scores.shape)\n",
        "\n",
        "    #print('\\n####################')\n",
        "    print('\\n######## Category: XR_HUMERUS #######')\n",
        "    \n",
        "    \n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('roc_auc=', roc_auc)\n",
        "    max_auc = max(roc_auc, max_auc)\n",
        "    print('max_auc=', max_auc)\n",
        "    \n",
        "    print(len(valid_dataloader.dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "torch.Size([64, 128]) torch.Size([64, 128])\n",
            "z_distance= tensor(0.0002, device='cuda:0')\n",
            "torch.Size([288])\n",
            "\n",
            "######## Category: XR_HUMERUS #######\n",
            "roc_auc= 0.912128877646119\n",
            "max_auc= 0.912128877646119\n",
            "288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVYgEL-v58fU"
      },
      "source": [
        "## 6.XR_SHOULDER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2lMxN3F58fV"
      },
      "source": [
        "# change the path here to shared drive\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)    #scs\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)\n",
        "\n",
        "\n",
        "mura_valid_df = customDf('/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/MURA-v1.1/valid_image_paths.csv', 'XR_SHOULDER', None)\n",
        "valid_dataset = MURA_dataset(mura_valid_df, '/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/', transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsCmGKy-58fV"
      },
      "source": [
        "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw0LsaQ258fV"
      },
      "source": [
        "### Metric Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e2DXGoe58fV"
      },
      "source": [
        "from torch.nn.functional import mse_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "47uNrIOg58fV",
        "outputId": "bf7300b6-397f-477d-a3b2-41278809ea33"
      },
      "source": [
        "\n",
        "\n",
        "n_epochs = 5001\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 128\n",
        "img_size = 64\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "abnormal_class = 0\n",
        "#device = 'cuda' \n",
        "out = '/content/drive/Shared drives/MeanSquare-Drive/RL-Project/AnoGAN/models/anoGAN-ckpts-XR_SHOULDER/' #scs- change here for all cat\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "max_auc = 0\n",
        "\n",
        "\n",
        "generator = Generator(dim = 64, zdim=latent_dim, nc=channels)\n",
        "discriminator = Discriminator(dim = 64, zdim=latent_dim, nc=channels,out_feat=True)\n",
        "encoder = Encoder(dim = 64, zdim=latent_dim, nc=channels)\n",
        "\n",
        "generator.load_state_dict(torch.load(out+'G_epoch5000.pt'))\n",
        "discriminator.load_state_dict(torch.load(out+'D_epoch5000.pt'))\n",
        "generator.to(device)\n",
        "encoder.to(device)\n",
        "discriminator.to(device)\n",
        "with torch.no_grad():\n",
        "    labels = torch.zeros(size=(len(valid_dataloader.dataset),),\n",
        "                                        dtype=torch.long, device=device)\n",
        "\n",
        "    scores = torch.empty(\n",
        "                size=(len(valid_dataloader.dataset),),\n",
        "                dtype=torch.float32,\n",
        "                device=device)\n",
        "    for i, (imgs, lbls) in enumerate(valid_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "\n",
        "            labels[i*batch_size:(i+1)*batch_size].copy_(lbls)\n",
        "            emb_query = encoder(imgs)\n",
        "            fake_imgs = generator(emb_query)\n",
        "            emb_fake = encoder(fake_imgs)\n",
        "\n",
        "            image_feats  = discriminator(imgs)\n",
        "            recon_feats = discriminator(fake_imgs)\n",
        "                \n",
        "            diff = imgs-fake_imgs\n",
        "            \n",
        "            image1_tensor= diff[0]\n",
        "           \n",
        "            im = tensor2im(imgs)\n",
        "            plt.imshow(im)\n",
        "            \n",
        "            im2 = tensor2im(fake_imgs)\n",
        "            plt.imshow(im2)\n",
        "            \n",
        "            im3 = tensor2im(diff)\n",
        "            plt.imshow(im3)\n",
        "            print(im.shape)\n",
        "            print(im3.shape)\n",
        "            #break   \n",
        "            \n",
        "            image_distance = torch.mean(torch.pow(imgs-fake_imgs, 2), dim=[1,2,3])\n",
        "            feat_distance = torch.mean(torch.pow(image_feats-recon_feats, 2), dim=1)\n",
        "            print(emb_query.shape, emb_fake.shape)\n",
        "            z_distance = mse_loss(emb_query, emb_fake)#mse_loss(emb_query, emb_fake)\n",
        "            #print z_distance\n",
        "            print('z_distance=',z_distance)\n",
        "            #print('hiiiiiiiii')\n",
        "            scores[i*batch_size:(i+1)*batch_size].copy_(feat_distance)\n",
        "            break\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    # scores = torch.mean(scores,)\n",
        "    scores = scores.cpu().squeeze()\n",
        "    print(scores.shape)\n",
        "\n",
        "    #print('\\n####################')\n",
        "    print('\\n######## Category: XR_SHOULDER #######')\n",
        "    \n",
        "    \n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('roc_auc=', roc_auc)\n",
        "    max_auc = max(roc_auc, max_auc)\n",
        "    print('max_auc=', max_auc)\n",
        "    \n",
        "    print(len(valid_dataloader.dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "torch.Size([64, 128]) torch.Size([64, 128])\n",
            "z_distance= tensor(6.9595e-05, device='cuda:0')\n",
            "torch.Size([563])\n",
            "\n",
            "######## Category: XR_SHOULDER #######\n",
            "roc_auc= 0.8893190921228304\n",
            "max_auc= 0.8893190921228304\n",
            "563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkd-f4N94Qdb"
      },
      "source": [
        "## 7.XR_WRIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNJ_uzsz4Qdb"
      },
      "source": [
        "# change the path here to shared drive\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "#transforms = transform(False, True, True, True, True, True, True, False)\n",
        "transforms = transform(False, True, True, True, True, True, True, False)\n",
        "\n",
        "#mura_valid_df = customDf('../datasets/MURA-v1.1/valid_image_paths.csv', 'XR_HUMERUS', None)    #scs\n",
        "#valid_dataset = MURA_dataset(mura_valid_df, '../datasets/', transforms)\n",
        "\n",
        "\n",
        "mura_valid_df = customDf('/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/MURA-v1.1/valid_image_paths.csv', 'XR_WRIST', None)\n",
        "valid_dataset = MURA_dataset(mura_valid_df, '/content/drive/Shared drives/MeanSquare-Drive/Advanced-DeepLearning/', transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmgAosXA4Qdb"
      },
      "source": [
        "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=64,shuffle=True,num_workers=4,drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFm720FG4Qdb"
      },
      "source": [
        "### Metric Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bndYvqeO4Qdb"
      },
      "source": [
        "from torch.nn.functional import mse_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "659JsU8u4Qdb",
        "outputId": "2bc1bc9b-86d0-4dda-bd86-7d2b088faa38"
      },
      "source": [
        "\n",
        "\n",
        "n_epochs = 5001\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 128\n",
        "img_size = 64\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "abnormal_class = 0\n",
        "#device = 'cuda' \n",
        "out = '/content/drive/Shared drives/MeanSquare-Drive/RL-Project/AnoGAN/models/anoGAN-ckpts-XR_WRIST/' #scs- change here for all cat\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "max_auc = 0\n",
        "\n",
        "\n",
        "generator = Generator(dim = 64, zdim=latent_dim, nc=channels)\n",
        "discriminator = Discriminator(dim = 64, zdim=latent_dim, nc=channels,out_feat=True)\n",
        "encoder = Encoder(dim = 64, zdim=latent_dim, nc=channels)\n",
        "\n",
        "generator.load_state_dict(torch.load(out+'G_epoch5000.pt'))\n",
        "discriminator.load_state_dict(torch.load(out+'D_epoch5000.pt'))\n",
        "generator.to(device)\n",
        "encoder.to(device)\n",
        "discriminator.to(device)\n",
        "with torch.no_grad():\n",
        "    labels = torch.zeros(size=(len(valid_dataloader.dataset),),\n",
        "                                        dtype=torch.long, device=device)\n",
        "\n",
        "    scores = torch.empty(\n",
        "                size=(len(valid_dataloader.dataset),),\n",
        "                dtype=torch.float32,\n",
        "                device=device)\n",
        "    for i, (imgs, lbls) in enumerate(valid_dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            lbls = lbls.to(device)\n",
        "\n",
        "            labels[i*batch_size:(i+1)*batch_size].copy_(lbls)\n",
        "            emb_query = encoder(imgs)\n",
        "            fake_imgs = generator(emb_query)\n",
        "            emb_fake = encoder(fake_imgs)\n",
        "\n",
        "            image_feats  = discriminator(imgs)\n",
        "            recon_feats = discriminator(fake_imgs)\n",
        "                \n",
        "            diff = imgs-fake_imgs\n",
        "            \n",
        "            image1_tensor= diff[0]\n",
        "           \n",
        "            im = tensor2im(imgs)\n",
        "            plt.imshow(im)\n",
        "            \n",
        "            im2 = tensor2im(fake_imgs)\n",
        "            plt.imshow(im2)\n",
        "            \n",
        "            im3 = tensor2im(diff)\n",
        "            plt.imshow(im3)\n",
        "            print(im.shape)\n",
        "            print(im3.shape)\n",
        "            #break   \n",
        "            \n",
        "            image_distance = torch.mean(torch.pow(imgs-fake_imgs, 2), dim=[1,2,3])\n",
        "            feat_distance = torch.mean(torch.pow(image_feats-recon_feats, 2), dim=1)\n",
        "            print(emb_query.shape, emb_fake.shape)\n",
        "            z_distance = mse_loss(emb_query, emb_fake)#mse_loss(emb_query, emb_fake)\n",
        "            #print z_distance\n",
        "            print('z_distance=',z_distance)\n",
        "            #print('hiiiiiiiii')\n",
        "            scores[i*batch_size:(i+1)*batch_size].copy_(feat_distance)\n",
        "            break\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    # scores = torch.mean(scores,)\n",
        "    scores = scores.cpu().squeeze()\n",
        "    print(scores.shape)\n",
        "\n",
        "    #print('\\n####################')\n",
        "    print('\\n######## Category: XR_WRIST #######')\n",
        "    \n",
        "    \n",
        "    # True/False Positive Rates.\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('roc_auc=', roc_auc)\n",
        "    max_auc = max(roc_auc, max_auc)\n",
        "    print('max_auc=', max_auc)\n",
        "    \n",
        "    print(len(valid_dataloader.dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "(32, 32, 3)\n",
            "torch.Size([64, 128]) torch.Size([64, 128])\n",
            "z_distance= tensor(3.8494e-05, device='cuda:0')\n",
            "torch.Size([659])\n",
            "\n",
            "######## Category: XR_WRIST #######\n",
            "roc_auc= 0.9753515301902399\n",
            "max_auc= 0.9753515301902399\n",
            "659\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}